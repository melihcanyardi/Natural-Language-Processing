{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The nlp object\n",
    "\n",
    "At the center of spaCy is the object containing the processing pipeline. We usually call this variable **nlp**.\n",
    "\n",
    "You can use the **nlp** object like a function to analyze text.\n",
    "- It contains all the different components in the processing pipeline.\n",
    "- It also includes language-specific rules used for tokenizing the text into words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Doc object\n",
    "\n",
    "When you process a text with the nlp object, spaCy creates a **Doc** object, short for \"document\".\n",
    "\n",
    "The **Doc** lets you access information about the text in a structured way and no information is lost.\n",
    "\n",
    "The **Doc** behaves like a normal Python sequence, and lets you iterate over its tokens, or get token by its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Created by processing a string f text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Token object\n",
    "\n",
    "**Token** objects represent the tokens in a document. For example, a word or a punctuation character.\n",
    "\n",
    "To get a **Token** at a specific position, you can index into the **Doc**.\n",
    "\n",
    "**Token** objects also provide various attributes that let you access more information about the tokens. For example, the **.text** attribute returns the verbatim token text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Span object\n",
    "\n",
    "A **Span** object is a slice of the document consisting of one or more tokens. It's only a view of the **Doc**, it doesn't contain any data itself.\n",
    "\n",
    "To create a **Span**, you can use Python's slice notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# A slice from the Doc is a Span object\n",
    "span = doc[1:4]\n",
    "\n",
    "# Get the span text via the .text object\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical attributes\n",
    "\n",
    "**Lexical attributes** refer to the entry in the vocabulary and don't depend on the token's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4]\n",
      "Text:     ['It', 'costs', '$', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, True]\n",
      "like_num: [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It costs $5.\")\n",
    "\n",
    "print('Index:   ', [token.i for token in doc]) # \"i\" is the index of the token within the parent document\n",
    "\n",
    "print('Text:    ', [token.text for token in doc]) # \"text\" returns the token text\n",
    "\n",
    "print('is_alpha:', [token.is_alpha for token in doc]) # return boolean values indicating whether\n",
    "                                                      # the token consists of alphanumeric characters\n",
    "\n",
    "print('is_punct:', [token.is_punct for token in doc]) # return boolean values indicating whether\n",
    "                                                      # the token is punctuation\n",
    "\n",
    "print('like_num:', [token.like_num for token in doc]) # return boolean values indicating whether\n",
    "                                                      # the token resembles a number (e.g. '10', '1', '0', 'ten')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are statistical models?\n",
    "\n",
    "Statistical models:\n",
    "- Enable spaCy to make predictions _in context_\n",
    "    - Part-of-speech tags\n",
    "    - Syntactic dependencies\n",
    "    - Named entities\n",
    "- Trained on large datasets of labeled example texts\n",
    "- Can be updated with more examples to fine-tune predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Packages\n",
    "\n",
    "spaCy provides a number of pre-trained model packages you can download.\n",
    "\n",
    "For example, the \"**en_core_web_sm**\" package is a small English model that supports all core capabilities and is trained on web text.\n",
    "\n",
    "The **spacy.load()** method loads a model package by name and returns an nlp object.\n",
    "\n",
    "The package provides:\n",
    "- **Binary weights** that enable spaCy to make predictions.\n",
    "- **Vocabulary**\n",
    "- **Meta information** (language, pipeline) to tell spaCy which language class to use and how to configure the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Part-of-speech Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spaCy, attributes that return string usually end with an underscore (e.g. **.pos_**). Attributes without the underscore return an ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Syntactic Dependencies\n",
    "\n",
    "The \"**.dep_**\" attribute returns the predicted dependency label.\n",
    "\n",
    "The \"**.head**\" attribute returns the syntactic head token. You can also think of it as the parent token this word is attached to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To describe syntactic dependencies, spaCy uses a standardized label scheme.\n",
    "\n",
    "| Label | Description | Example |\n",
    "| --- | --- | --- |\n",
    "| nsubj | nominal subject | She |\n",
    "| dobj | direct object | pizza |\n",
    "| det | determiner (article) | the |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Named Entities\n",
    "\n",
    "**Named entities** are \"real world objects\" that are assigned a name. For example, a person, an organization, or a country.\n",
    "\n",
    "The **doc.ents** property lets you access the named entities predicted by the model. It returns an iterator of **Span** objects, so we can print the entity text (**ent.text**) and the entity label (**ent.label_**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip: the explain method\n",
    "\n",
    "To get definitions for the most common tags and labels, you can use the **spacy.explain()** helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based matching\n",
    "\n",
    "spaCy's **Matcher** lets you write rules to find words and phrases in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not just regular expressions?\n",
    "\n",
    "- Compared to regular expressions, the **Matcher** works with **Doc** and **Token** objects instead of only strings.\n",
    "- It is also more flexible: you can search for texts, but also other lexical attributes.\n",
    "- You can even write rules that use the model's predictions. For example find the word \"duck\" only if it is a verb, not a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match patterns\n",
    "\n",
    "**Match patterns** are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.\n",
    "\n",
    "- Match exact token texts:\n",
    "    - <code style=\"background:#RRGGBB\">\\[{'ORTH': 'iPhone'}, {'ORTH': 'X'}\\]</code>\n",
    "- Match lexical attributes:\n",
    "    - <code style=\"background:#RRGGBB\">\\[{'LOWER': 'iphone'}, {'LOWER': 'x'}\\]</code>\n",
    "- Match any token attributes\n",
    "    - <code style=\"background:#RRGGBB\">\\[{'LEMMA': 'buy'}, {'POS': 'NOUN'}\\]</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{'ORTH': 'iPhone'}, {'ORTH': 'X'}]\n",
    "matcher.add('IPHONE_PATTERN', [pattern], on_match=None)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <code style=\"background:#RRGGBB\">match_id:</code> hash value of the pattern name\n",
    "- <code style=\"background:#RRGGBB\">start:</code> start index of matched span\n",
    "- <code style=\"background:#RRGGBB\">end:</code> end index of matched span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': 'fifa'},\n",
    "    {'LOWER': 'world'},\n",
    "    {'LOWER': 'cup'},\n",
    "    {'IS_PUNCT': True}\n",
    "]\n",
    "matcher.add('FIFA_PATTERN', [pattern], on_match=None)\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching other token attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "matcher.add('LOVE_PATTERN', [pattern], on_match=None)\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using operators and quantifiers\n",
    "\n",
    "**Operators** and **quantifiers** lets you define how often a token should be matched. They can be added using the \"**OP**\" key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [\n",
    "    {'LEMMA': 'buy'},\n",
    "    {'POS': 'DET', 'OP': '?'}, # optional: match 0 or 1 times\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "matcher.add('BUY_PATTERN', [pattern], on_match=None)\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the \"**?**\" operator makes the determiner token optional, so it will match a taken with the lemma \"buy\", an optional article, and a noun.\n",
    "\n",
    "\"**OP**\" can have one of four values:\n",
    "\n",
    "| | Description |\n",
    "| --- | --- |\n",
    "| <code style=\"background:#RRGGBB\">{'OP': '!'}</code> | Negation: match 0 times |\n",
    "| <code style=\"background:#RRGGBB\">{'OP': '?'}</code> | Optional: match 0 or 1 times |\n",
    "| <code style=\"background:#RRGGBB\">{'OP': '+'}</code> | Match 1 or more times |\n",
    "| <code style=\"background:#RRGGBB\">{'OP': '*'}</code> | Match 0 or more times|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Writing match patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"After making the iOS update you won't notice a radical system-wide redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of iOS 11's furniture remains the same as in iOS 10. But you will discover some tweaks once you delve a little deeper.\")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{'TEXT': 'iOS'}, {'IS_DIGIT': True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('IOS_VERSION_PATTERN', [pattern])\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n",
      "Match found: download Winzip\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"i downloaded Fortnite on my laptop and can't open the game at all. Help? so when I was downloading Minecraft, I got the Windows version where it is the '.zip' folder and I used the default program to unpack it... do I also need to download Winzip?\")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{'LEMMA': 'download'}, {'POS': 'PROPN'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('DOWNLOAD_THINGS_PATTERN', [pattern])\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('ADJ_NOUN_PATTERN', [pattern])\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-scale data analysis with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures: Vocab, Lexemes and StringStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared vocab and string store\n",
    "\n",
    "spaCy stores all shared data in a vocabulary, the **Vocab**. This includes words, but also the labels, schemes for tags, and entities.\n",
    "\n",
    "To save memory, spaCy encodes all strings to **hash values**. If a word occurs more than once, we don't need to save it every time. Instead, spaCy uses a hash function to generate an ID and stores the string only once in the **StringStore**, which is available in **nlp.vocab.strings**\n",
    "\n",
    "**String store** is a **lookup table** that works in both directions. You can look up a string and get its hash, and look up a hash to get its string value. Internally, spaCy only communicates in hash IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n",
      "coffee\n"
     ]
    }
   ],
   "source": [
    "coffee_hash = nlp.vocab.strings['coffee']\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "print(coffee_hash)\n",
    "print(coffee_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hash IDs can't be reversed, though. If a word is not in a vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raises an error if we haven't seen the string before\n",
    "string = nlp.vocab.strings[3197928453018144401]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the hash for a string, we can look it up in **nlp.vocab.strings**\n",
    "\n",
    "To get a string representation of a hash, we can look up the hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n",
      "string value: coffee\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value:', nlp.vocab.strings['coffee'])\n",
    "print('string value:', nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Doc** object also exposes its vocab and strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value:', doc.vocab.strings['coffee'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexemes: entries in the vocabulary\n",
    "\n",
    "**Lexemes** are context-independent entries in the vocabulary.\n",
    "\n",
    "You can get a **lexeme** by looking up a string or a hash ID in the vocab.\n",
    "\n",
    "**Lexemes** expose attributes, just like tokens.\n",
    "\n",
    "They hold **context-indepenent** information about a word:\n",
    "- Word text: **lexeme.text** and **lexeme.orth** (the hash)\n",
    "- Lexical attributes like **lexeme.is_alpha**\n",
    "- **Not** context-dependent part-of-speech tags, dependencies or entity labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "# print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures: Doc, Span and Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Doc object\n",
    "\n",
    "The **Doc** is one of the central data structures in spaCy.\n",
    "\n",
    "It is created automatically when you process a text with the **nlp** object. But you can also instantiate the class manually. After creating the **nlp** object, we can import the **Doc** class from **spacy.tokens**.\n",
    "\n",
    "The **Doc** class takes three arguments: the share vocab, the words and the spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Span object\n",
    "\n",
    "A **Span** is a slice of a **Doc** consisting of one or more tokens.\n",
    "\n",
    "The **Span** takes at least three arguments: the **Doc** it refers to, and the start and end (exclusive) index of the **Span**.\n",
    "\n",
    "To create a **Span** manually, we can also import the class from **spacy.tokens**. We can then instantiate it with the **Doc** and the **Span**'s start and end index.\n",
    "\n",
    "To add an **entity label** to the **Span**, we can pass in the label name as the **label** argument. For consistency, we usually write label names in capital letters.\n",
    "\n",
    "The **doc.ents** are writable so we can add entities manually by overwriting it with a list of spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices\n",
    "\n",
    "- <code style=\"background:#RRGGBB\">Doc</code> and <code style=\"background:#RRGGBB\">Span</code> are very powerful and hold references and relationships of words and sentences\n",
    "    - Convert result to strings as late as possible\n",
    "    - Use token attributes if available - for example, <code style=\"background:#RRGGBB\">token.i</code> for the token index\n",
    "- Don't forget to pass in the shared <code style=\"background:#RRGGBB\">vocab</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors and semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing semantic similarity\n",
    "- <code style=\"background:#RRGGBB\">spaCy</code> can compare two objects (documents, spans, or single tokens) and predict similarity\n",
    "- <code style=\"background:#RRGGBB\">Doc.similarity()</code>, <code style=\"background:#RRGGBB\">Span.similarity()</code>, <code style=\"background:#RRGGBB\">Token.similarity()</code>\n",
    "    - Take another object and return a similarity score (<code style=\"background:#RRGGBB\">0</code> to <code style=\"background:#RRGGBB\">1</code>)\n",
    "- **Important:** needs a model that has word vectors included, for example:\n",
    "    - **YES**: <code style=\"background:#RRGGBB\">en_core_web_md</code> (medium model)\n",
    "    - **YES**: <code style=\"background:#RRGGBB\">en_core_web_lg</code> (large model)\n",
    "    - **NO**: <code style=\"background:#RRGGBB\">en_core_web_sm</code> (small model)\n",
    "\n",
    "So, if you want to use vectors, always go with a model that ends in \"md\" or \"lg\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627204117787385\n"
     ]
    }
   ],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7369546\n"
     ]
    }
   ],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the similarity methods to compare different types of objects. For example, a document and a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32531983166759537\n"
     ]
    }
   ],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6199092090831612\n"
     ]
    }
   ],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does spaCy predict similarity?\n",
    "\n",
    "- Similarity is determined using **word vectors**\n",
    "- Multi-dimensional meaning of representations of words\n",
    "- Generated using an algorith like **Word2Vec** and lots of text\n",
    "- Can be added to spaCy's statistical models\n",
    "- Default: **cosine similarity**, but can be adjusted\n",
    "- <code style=\"background:#RRGGBB\">Doc</code> and <code style=\"background:#RRGGBB\">Span</code> vectors default of token vectors\n",
    "- Short phrases are better than long documents with many irrelevant words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
      " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
      " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
      "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
      "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
      " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
      "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
      " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
      "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
      "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
      " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
      " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
      "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
      "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
      " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
      "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
      " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
      " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
      "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
      "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
      "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
      "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
      "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
      " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
      " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
      "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
      "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
      "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
      "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
      " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
      "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
      " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
      "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
      " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
      "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
      "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
      " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
      "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
      "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
      " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
      " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
      "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
      " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
      "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
      " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
      " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
      "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
      "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
      "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
      " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
     ]
    }
   ],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"I have a banana\")\n",
    "\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity depends on the application context\n",
    "\n",
    "- Useful for many applications: recommendation systems, flagging duplicates, etc.\n",
    "- There's no objective definition of \"similarity\"\n",
    "- Depends on the context and what application needs to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9501447503553421\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining models and rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical predictions vs. rules\n",
    "\n",
    "|  | Statistical models | Rule-based systems |\n",
    "| --- | --- | --- |\n",
    "| **Use cases** | application needs to _generalize_ based on examples | dictionary with finite number of examples |\n",
    "| **Real-world examples** | product names, person names, subject/object relationships | countries of the world, cities, drug names, dog breeds |\n",
    "| **spaCy features** | entity recognizer, dependency parser, part-of-speech tagger | tokenizer, <code style=\"background:#RRGGBB\">Matcher</code>, <code style=\"background:#RRGGBB\">PhraseMatcher</code> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Rule-based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: love cats\n",
      "Matched span: very happy\n",
      "Matched span: very very happy\n"
     ]
    }
   ],
   "source": [
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "pattern = [{'LEMMA': 'love', 'POS': 'VERB'}, {'LOWER': 'cats'}]\n",
    "matcher.add('LOVE_CATS', [pattern])\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{'TEXT': 'very', 'OP': '+'}, {'TEXT': 'happy'}]\n",
    "matcher.add('VERY_HAPPY', [pattern])\n",
    "\n",
    "# Calling a matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches returned by Matcher\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding statistical predictions\n",
    "\n",
    "**Span** objects give us access to the original document and all other token attributes and linguistic features predicted by the model.\n",
    "\n",
    "For example, we can get the **Span**'s **root token**. If the **Span** consists of more than one token, this will be the token that decides the category of the phrase. For example, the root of \"Golden Retriever\" is \"Retriever\".\n",
    "\n",
    "We can also find the **head token** of the root. This is the syntactic \"**parent**\" that governs the phrase - in this case, the verb \"have\".\n",
    "\n",
    "Finally, we can look at the previous token and its attributes. In this case, it's a determiner, the article \"a\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Rood head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', [[{'LOWER': 'golden'}, {'LOWER': 'retriever'}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Rood head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient phrase matching\n",
    "\n",
    "The **PhraseMatcher** is another helpful tool to find sequences of words in your data. It performs a keyword search on the document, but instead of only finding strings, it gives you direct access to the tokens in context.\n",
    "\n",
    "- <code style=\"background:#RRGGBB\">PhraseMatcher</code> like regular expressions or keyword search - but with access to the tokens!\n",
    "- Takes <code style=\"background:#RRGGBB\">Doc</code> object as patterns\n",
    "- More efficient and faster than the <code style=\"background:#RRGGBB\">Matcher</code>\n",
    "- Great for matching large word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add('DOG', [pattern])\n",
    "\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when you call nlp?\n",
    "\n",
    "- First, the **tokenizer** is applied to turn the string of text into a **Doc** object.\n",
    "- Next, a series of pipeline components is applied to the **Doc** in order:\n",
    "    - **Tagger**\n",
    "    - **Parser**\n",
    "    - **Entity recognizer**\n",
    "- Finally, the processed **Doc** is returned so you can work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in pipeline components\n",
    "\n",
    "| Name | Description | Creates |\n",
    "| --- | --- | --- |\n",
    "| **tagger** | Part-of-speech tagger | <code style=\"background:#RRGGBB\">Token.tag</code> |\n",
    "| **parser** | Dependency parser | <code style=\"background:#RRGGBB\">Token.dep</code>, <code style=\"background:#RRGGBB\">Token.head</code>, <code style=\"background:#RRGGBB\">Doc.sents</code>, <code style=\"background:#RRGGBB\">Doc.noun_chuncks</code>|\n",
    "| **ner** | Named entity recognizer | <code style=\"background:#RRGGBB\">Doc.ents</code>, <code style=\"background:#RRGGBB\">Token.ent_iob</code>, <code style=\"background:#RRGGBB\">Token.ent_type</code> |\n",
    "| **textcat** | Text classifier | <code style=\"background:#RRGGBB\">Doc.cats</code>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the hood\n",
    "\n",
    "All models you can load into spaCy include several files and meta JSON.\n",
    "\n",
    "The meta defines things like the language and pipeline. This tells spaCy which components to instantiate.\n",
    "\n",
    "- Pipeline defined in model's <code style=\"background:#RRGGBB\">meta.json</code> in order\n",
    "- Built-in components need binary data to make predictions. The data is included in the model package and loaded into the component when you load the model.\n",
    "\n",
    "**meta.json** for **en_core_web_sm**\n",
    "```python\n",
    "{\n",
    "    \"lang\":\"en\",\n",
    "    \"name\":\"core_web_sm\",\n",
    "    \"pipeline\": [\"tagger\", \"parser\", \"ner\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline attributes\n",
    "\n",
    "- <code style=\"background:#RRGGBB\">nlp.pipe_names</code>: list of pipeline components\n",
    "- <code style=\"background:#RRGGBB\">nlp.pipeline</code>: list of <code style=\"background:#RRGGBB\">(name, component)</code> tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x00000123FC839720>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x00000123FE70A220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x00000123FE73D8E0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x00000123FE73D460>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000012393A00E40>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x0000012393A05840>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom pipeline components\n",
    "\n",
    "**Custom pipeline components** let you add your own function to the spaCy pipeline that is executed when you call the **nlp** object on a text - for example, to modify the **Doc** and add more data to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why custom components?\n",
    "\n",
    "- Make a function execute automatically when you call <code style=\"background:#RRGGBB\">nlp</code>\n",
    "- Add your own metadata to documents and tokens\n",
    "- Updating built-in attributes like <code style=\"background:#RRGGBB\">doc.ents</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a component\n",
    "- Function that takes a <code style=\"background:#RRGGBB\">doc</code>, modifies it and returns it\n",
    "- Can be added using the <code style=\"background:#RRGGBB\">nlp.add_pipe</code> method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.custom_component(doc)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@Language.component(\"my_component\")\n",
    "def custom_component(doc):\n",
    "    # Do something to the doc here\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"my_component\")\n",
    "#nlp.add_pipe(\"my_component\", last=True)\n",
    "#nlp.add_pipe(\"my_component\", first=True)\n",
    "#nlp.add_pipe(\"my_component\", before=\"ner\")\n",
    "#nlp.add_pipe(\"my_component\", before=\"tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify where to add the component in the pipeline, you can use the following keyword arguments:\n",
    "\n",
    "| Argument | Description | Examples |\n",
    "| --- | --- | --- |\n",
    "| <code style=\"background:#RRGGBB\">last</code> | If <code style=\"background:#RRGGBB\">True</code>, add last | <code style=\"background:#RRGGBB\">nlp.add_pipe(component, last=True)</code> |\n",
    "| <code style=\"background:#RRGGBB\">first</code> | If <code style=\"background:#RRGGBB\">True</code>, add first | <code style=\"background:#RRGGBB\">nlp.add_pipe(component, first=True)</code> |\n",
    "| <code style=\"background:#RRGGBB\">before</code> | Add before component | <code style=\"background:#RRGGBB\">nlp.add_pipe(component, before='ner')</code> |\n",
    "| <code style=\"background:#RRGGBB\">after</code> | Add after component | <code style=\"background:#RRGGBB\">nlp.add_pipe(component, after='tagger')</code> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: a simple component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "@Language.component(\"custom_component\")\n",
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    # Print the doc's length\n",
    "    print('Doc length:', len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print('Pipeline:', nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting custom attributes\n",
    "\n",
    "- Add custom metadata to documents, tokens and spans\n",
    "- The data can be added once, or it can be computed dynamically.\n",
    "- Accessible via <code style=\"background:#RRGGBB\">._</code> property\n",
    "```python\n",
    "doc._.title = 'My document'\n",
    "token._.is_color = True\n",
    "span._.has_color = False\n",
    "```\n",
    "- Attributes can be registered on the global <code style=\"background:#RRGGBB\">Doc</code>, <code style=\"background:#RRGGBB\">Token</code>, or <code style=\"background:#RRGGBB\">Span</code> using the <code style=\"background:#RRGGBB\">set_extension</code> method\n",
    "    - The first argument is the attribute name.\n",
    "    - Keyword arguments let you define how the value should be computed. In this case, it has a default value and can be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Set extensions on the Doc, Token and Span\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension attribute types\n",
    "1. Attribute extensions\n",
    "2. Property extensions\n",
    "3. Method extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute extensions\n",
    "\n",
    "**Attribute extensions** set a default value that can be overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set extension on the Token with default value\n",
    "Token.set_extension('is_color', default=False, force=True)\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "\n",
    "# Overwrite extension attribute value\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property extensions\n",
    "\n",
    "**Property extensions** work like properties in Python.\n",
    "- Define a getter function and an optional setter function.\n",
    "- Getter only called when you _retrieve_ the attribute value. This lets you compute the value dynamically and even take other custom attributes into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return token.text in colors\n",
    "\n",
    "# Set extension on the Token with getter\n",
    "Token.set_extension('is_color', getter=get_is_color, force=True)\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[3]._.is_color, '-', doc[3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <code style=\"background:#RRGGBB\">Span</code> extensions should almost always use a getter. Otherwise, you'd have to update every possible span ever by hand to set all the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - sky is blue\n",
      "False - The sky\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# Define getter function\n",
    "def get_has_color(span):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Span.set_extension('has_color', getter=get_has_color, force=True)\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[1:4]._.has_color, '-', doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, '-', doc[0:2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method extensions\n",
    "\n",
    "**Method extensions** make the extension attribute a callable method.\n",
    "\n",
    "- Assign a **function** that becomes available as an object method.\n",
    "- Let you pass **arguments** to the extension function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n",
      "False - cloud\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "    \n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension('has_token', method=has_token)\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token('blue'), '- blue')\n",
    "print(doc._.has_token('cloud'), '- cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Setting extension attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "# Register the Token extension attribute 'is_country' with the default value False\n",
    "Token.set_extension('is_country', default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "  \n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension('reversed', getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print('reversed:', token._.reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
    "Doc.set_extension('has_number', getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute \n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print('has_number:', doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return '<{tag}>{text}</{tag}>'.format(tag=tag, text=span.text)\n",
    "\n",
    "# Register the Span property extension 'to_html' with the method to_html\n",
    "Span.set_extension('to_html', method=to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name 'strong'\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html('strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Entities and extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\"In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing large volumes of texts\n",
    "\n",
    "If you need to process a lot of texts and generate a lot of **Doc** in a row, the <code style=\"background:#RRGGBB\">nlp.pipe</code> method can speed this up significantly.\n",
    "- Processes texts as a stream, yields <code style=\"background:#RRGGBB\">Doc</code> objects.\n",
    "- Much faster than calling <code style=\"background:#RRGGBB\">nlp</code> on each text, because it batches up the texts.\n",
    "\n",
    "In order to get a list of docs, remember to call the list method around it.\n",
    "\n",
    "**BAD:**\n",
    "```python\n",
    "docs = [nlp(text) for text in LOTS_OF_TEXTS]\n",
    "```\n",
    "\n",
    "**GOOD:**\n",
    "```python\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXTS))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing in context\n",
    "\n",
    "- Setting <code style=\"background:#RRGGBB\">as_tuples=True</code> on <code style=\"background:#RRGGBB\">nlp.pipe</code> lets you pass in <code style=\"background:#RRGGBB\">(text, context)</code> tuples\n",
    "- Yields <code style=\"background:#RRGGBB\">(doc, context)</code> tuples\n",
    "- Useful for associating metadata with the <code style=\"background:#RRGGBB\">doc</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "Add another text 16\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('Add another text', {'id': 2, 'page_number': 16})\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context['page_number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even add the context metadata to custom attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension('id', default=None)\n",
    "Doc.set_extension('page_number', default=None)\n",
    "\n",
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('Add another text', {'id': 2, 'page_number': 16})\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context['id']\n",
    "    doc._.page_number = context['page_number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using only the tokenizer\n",
    "\n",
    "Another common scenario: Sometimes you already have a model loaded to do other processing, but you only need the tokenizer for one particular text. Running the whole pipeline is unnecessarily slow, because you'll be getting a bunch of predictions from the model that you don't need.\n",
    "\n",
    "If you only need a tokenized **Doc** object, you can use the <code style=\"background:#RRGGBB\">nlp.make_doc</code> method instead, which takes the text and returns the <code style=\"background:#RRGGBB\">Doc</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp.make_doc(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling pipeline components\n",
    "\n",
    "- Use <code style=\"background:#RRGGBB\">nlp.disable_pipes</code> to temporarily disable one or more pipes. It takes a variable number of arguments, the string names of the pipeline components to disable.\n",
    "\n",
    "For example, if you only want to use the entity recognizer to process a document, you can temporarily disable the tagger and parser.\n",
    "- After the <code style=\"background:#RRGGBB\">with</code> block, the disabled pipeline components are automatically restored.\n",
    "- In the <code style=\"background:#RRGGBB\">with</code> block, spaCy will only run the remaining components.\n",
    "\n",
    "\n",
    "```python\n",
    "# Disable tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and updating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why updating the model?\n",
    "\n",
    "- Better results on your specific domain\n",
    "- Learn classification schemes specifically for your problem\n",
    "- Essential for text classification\n",
    "- Very useful for named entity recognition\n",
    "- Less critical for part-of-speech tagging and dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How training works\n",
    "1. **Initialize** the model weights randomly with <code style=\"background:#RRGGBB\">nlp.begin_training</code>\n",
    "2. **Predict** a few examples with the current weights by calling <code style=\"background:#RRGGBB\">nlp.update</code>\n",
    "3. **Compare** prediction with true labels\n",
    "4. **Calculate** how to change weights to improve predictions\n",
    "5. **Update** weights slightly\n",
    "6. Go back to 2.\n",
    "\n",
    "<img src=\"Training and Updating Models.png\">\n",
    "\n",
    "- **Training data**: Examples and their annotations.\n",
    "- **Text**: The input text the model should predict a label for.\n",
    "- **Label**: The label the model should predict.\n",
    "- **Gradient**: How to change the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Training the entity recognizer\n",
    "- The **entity recognizer** tags words and phrases in context\n",
    "    - This means that the training data needs to include texts, the entities they contain, and the entity labels\n",
    "- Each token can only be part of one entity\n",
    "- Examples need to come with context\n",
    "    - The easiest way to do this is to show the model a text and a list of character offsets.\n",
    "        ```python\n",
    "        (\"iPhone X is coming\", {'entities': [(0, 8, 'GADGET')]})\n",
    "        ```\n",
    "- Texts with no entities are also important\n",
    "    ```python\n",
    "    (\"I need a new phone! Any tips?\", {'entities': []})\n",
    "    ```\n",
    "- **Goal**: teach the model to generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training data\n",
    "- Examples of what we want the model to predict in context\n",
    "- Update an **existing model**: a few hundred to a few thousand examples\n",
    "- Train a **new category**: a few thousand to a million examples\n",
    "    - spaCy's English models: 2 million words\n",
    "- Usually created manually by human annotators\n",
    "- Can be semi-automated - for example, using spaCy's <code style=\"background:#RRGGBB\">Matcher</code>!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Creating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "TEXTS = ['How to preorder the iPhone X',\n",
    "         'iPhone X is coming',\n",
    "         'Should I pay $1,000 for the iPhone X?',\n",
    "         'The iPhone 8 reviews are here',\n",
    "         'Your iPhone goes up to 11 today',\n",
    "         'I need a new phone! Any tips?']\n",
    "\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{'LOWER': 'iphone'}, {'IS_DIGIT': True, 'OP': '?'}]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "matcher.add('GADGET', patterns = [pattern1, pattern2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to preorder the iPhone X [(4, 6, 'GADGET'), (4, 5, 'GADGET')]\n",
      "iPhone X is coming [(0, 2, 'GADGET'), (0, 1, 'GADGET')]\n",
      "Should I pay $1,000 for the iPhone X? [(7, 9, 'GADGET'), (7, 8, 'GADGET')]\n",
      "The iPhone 8 reviews are here [(1, 2, 'GADGET'), (1, 3, 'GADGET')]\n",
      "Your iPhone goes up to 11 today [(1, 2, 'GADGET')]\n",
      "I need a new phone! Any tips? []\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Find the matches in the doc\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    # Get a list of (start, end, label) tuples of matches in the text\n",
    "    entities = [(start, end, 'GADGET') for match_id, start, end in matches]\n",
    "    print(doc.text, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET'), (20, 26, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET'), (0, 6, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET'), (28, 34, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 10, 'GADGET'), (4, 12, 'GADGET')]})\n",
      "('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, 'GADGET') for span in spans]\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {'entities': entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    \n",
    "print(*TRAINING_DATA, sep='\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The steps of a training loop\n",
    "\n",
    "The **training loop** is a series of steps that's performed to train or update a model.\n",
    "\n",
    "1. **Loop** for a number of times.\n",
    "2. **Shuffle** the training data.\n",
    "3. **Divide** the data into batches.\n",
    "4. **Update** the model for each batch.\n",
    "5. **Save** the updated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example loop\n",
    "\n",
    "```python\n",
    "# Loop for 10 iterations\n",
    "for i in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    # Create batches and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, 6):\n",
    "        # Split the batch in texts and annotations\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)\n",
    "        \n",
    "# Save the model\n",
    "nlp.to_disk(path_to_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating an existing model\n",
    "\n",
    "- Improve the predictions on new data\n",
    "- Especially useful to improve existing categories, like <code style=\"background:#RRGGBB\">PERSON</code>\n",
    "- Also possible to add new categories\n",
    "- Be careful and make sure the model doesn't \"forget\" the old ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a new pipeline from scratch\n",
    "\n",
    "In this example, we start off with a blank English model using the **spacy.blank()** method. The blank model doesn't have any pipeline components, only the language data and tokenization rules.\n",
    "\n",
    "```python\n",
    "# Start with blank English model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create blank entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add a new label\n",
    "ner.add_label('GER')\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Train for 10 iterations\n",
    "for itn in range(10):\n",
    "    random.shuffle(examples)\n",
    "    # Divide examples into batches\n",
    "    for batch in spacy.util.minibatch(examples, size=2):\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Models can \"forget\" things\n",
    "- Existing model can overfit on new data\n",
    "    - e.g.: if you only update it with <code style=\"background:#RRGGBB\">WEBSITE</code>, it can \"unlearn\" what a <code style=\"background:#RRGGBB\">PERSON</code> is\n",
    "- Also known as \"catastrophic forgetting\" problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Mix in previously correct predictions\n",
    "- For example, if you're training <code style=\"background:#RRGGBB\">WEBSITE</code>, also include examples of <code style=\"background:#RRGGBB\">PERSON</code>\n",
    "- Run existing spaCy model over data and extract all other relevant entities\n",
    "\n",
    "**BAD:**\n",
    "```python\n",
    "TRAINING DATA = [\n",
    "    ('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]})\n",
    "]\n",
    "```\n",
    "\n",
    "**GOOD:**\n",
    "```python\n",
    "TRAINING DATA = [\n",
    "    ('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]})\n",
    "    ('Obama is a person', {'entities': [(0, 5, 'PERSON')]})\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Models can't learn everything\n",
    "- spaCy's models make predictions based on **local context**\n",
    "- Model can struggle to learn if decision is difficult to make based on context\n",
    "- Label scheme needs to be consistent and not too specific\n",
    "    - For example: <code style=\"background:#RRGGBB\">CLOTHING</code> is better than <code style=\"background:#RRGGBB\">ADULT_CLOTHING</code> and <code style=\"background:#RRGGBB\">CHILDRENS_CLOTHING</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Plan your label scheme carefully\n",
    "- Pick categories that are reflected in local context\n",
    "- More generic is better than too specific\n",
    "- Use rules to go from generic labels to specific categories\n",
    "\n",
    "**BAD:**\n",
    "```python\n",
    "LABELS = ['ADULT_SHOES', 'CHILDRENS_SHOES', 'BANDS_I_LIKE']\n",
    "```\n",
    "\n",
    "**GOOD:**\n",
    "```python\n",
    "LABELS = ['CLOTHING', 'BANDS']\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
