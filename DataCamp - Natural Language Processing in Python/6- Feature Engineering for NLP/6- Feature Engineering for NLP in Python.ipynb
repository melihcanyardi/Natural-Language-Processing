{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Features and Readability Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NLP feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical data\n",
    "\n",
    "For any ML algorithm, data fed into it must be in tabular form and all the training features must be numerical.\n",
    "\n",
    "|**sepal length**|**sepal width**|**petal length**|**petal width**|   **class**   |\n",
    "|----------------|---------------|----------------|---------------|---------------|\n",
    "|       6.3      |      2.9      |      5.6       |      1.8      |Iris-virginica |\n",
    "|       4.9      |      3.0      |      1.4       |      0.2      |  Iris-setosa  |\n",
    "|       5.6      |      2.9      |      3.6       |      1.3      |Iris-versicolor|\n",
    "|       6.0      |      2.7      |      5.1       |      1.6      |Iris-versicolor|\n",
    "|       7.2      |      3.6      |      6.1       |      2.5      |Iris-virginica |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "ML algorithms can also work with categorical data provided they are converted into numerical form through **one-hot encoding**.\n",
    "\n",
    "|**sex**|**one-hot encoding**|**sex_female**|**sex-male**|\n",
    "|-------|--------------------|--------------|------------|\n",
    "|female|-->|1|0|\n",
    "|male|-->|0|1|\n",
    "|female|-->|1|0|\n",
    "|male|-->|0|1|\n",
    "|female|-->|1|0|\n",
    "|...|...|...|...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Perform one-hot encoding on the 'sex' feature of df\n",
    "df = pd.get_dummies(df, columns=['sex'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual data\n",
    "#### Movie Review Dataset\n",
    "\n",
    "|**review**|**class**|\n",
    "|----------|---------|\n",
    "| This movie is for dog lovers. A very poignant... | positive |\n",
    "| The movie is forgettable. The plot lacked... | negative |\n",
    "| A truly amazing movie about dogs. A gripping... | positive |\n",
    "\n",
    "Consider a movie reviews dataset. This data cannot be utilized by any machine learning algorithm. The training feature 'review' isn't numerical. Neither is it categorical to perform one-hot encoding on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing\n",
    "\n",
    "We need to perform two steps to make this dataset suitable for ML.\n",
    "\n",
    "1- **Standardize the text**:\n",
    "- Converting to lowercase\n",
    "    * Example: <code style=\"background:#RRGGBB\">Reduction</code> to <code style=\"background:#RRGGBB\">reduction</code>\n",
    "- Converting to base-form\n",
    "    * Example: <code style=\"background:#RRGGBB\">reduction</code> to <code style=\"background:#RRGGBB\">reduce</code>\n",
    "    \n",
    "### Vectorization\n",
    "After preprocessing, the reviews are converted into a set of numerical training features through a process known as **vectorization**.\n",
    "\n",
    "After vectorization, our original review dataset gets converted into something like this:\n",
    "\n",
    "|**0**|**1**|**2**|**...**|**n**|   **class**   |\n",
    "|-----|-----|-----|-------|-----|---------------|\n",
    "|       0.03      |      0.71      |      0.00       |      ...      |       0.22       |       positive       |\n",
    "|       0.45      |      0.00      |      0.03       |      ...      |       0.19       |       negative       |\n",
    "|       0.14      |      0.18      |      0.00       |      ...      |       0.45       |       positive       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic features\n",
    "\n",
    "We can also extract certain basic features from text:\n",
    "- Number of words\n",
    "- Number of characters\n",
    "- Average length of words\n",
    "\n",
    "\n",
    "When working with niche data such as Tweets, it may also be useful to know how many hashtags have been used in a tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "\n",
    "So far, we have seen how to extract features out of an entire body of text.\n",
    "\n",
    "Some NLP applications may require you to extract features for individual words. For instance, you may want to do **parts-of-speech tagging (POS tagging)** to know the different parts-of-speech present in your text as shown.\n",
    "\n",
    "| **Word** | **POS** |\n",
    "|----------|---------|\n",
    "|     I    | Pronoun |\n",
    "|   have   |   Verb  |\n",
    "|     a    | Article |\n",
    "|    dog   |   Noun  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "You may also want to know perform **named entity recognition** to find out if a particular noun is referring to a person, organization or country.\n",
    "\n",
    "**Example**: \"Brian works at DataCamp.\"\n",
    "\n",
    "| **Noun** |    **NER**     |\n",
    "|----------|----------------|\n",
    "|   Brian  |   **Person**   |\n",
    "| DataCamp |**Organization**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts covered\n",
    "\n",
    "- Text Preprocessing\n",
    "- Basic Features\n",
    "- Word Features\n",
    "- Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = pd.Series([29.0000, 0.9167, 2.0000, 30.0000, 25.0000, 48.0000, 63.0000, 39.0000, 53.0000, 71.0000, 47.0000, 18.0000, 24.0000, 26.0000, 80.0000], name='feature 1')\n",
    "feature_2 = pd.Series([0, 1, 1, 1, 1, 0, 1, 0, 2, 0, 1, 1, 0, 0, 0], name='feature 2')\n",
    "feature_3 = pd.Series([0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], name='feature 3')\n",
    "feature_4 = pd.Series([29.0000, 0.9167, 2.0000, 30.0000, 25.0000, 48.0000, 63.0000, 39.0000, 53.0000, 71.0000, 47.0000, 18.0000, 24.0000, 26.0000, 80.0000], name='feature 4')\n",
    "feature_5 = pd.Series(['female', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'female', 'female', 'male'], name='feature 5')\n",
    "label = pd.Series([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1], name='label')\n",
    "\n",
    "df1 = pd.DataFrame([feature_1, feature_2, feature_3, feature_4, feature_5, label]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature 1', 'feature 2', 'feature 3', 'feature 4', 'feature 5',\n",
      "       'label'],\n",
      "      dtype='object')\n",
      "Index(['feature 1', 'feature 2', 'feature 3', 'feature 4', 'label',\n",
      "       'feature 5_female', 'feature 5_male'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature 1</th>\n",
       "      <th>feature 2</th>\n",
       "      <th>feature 3</th>\n",
       "      <th>feature 4</th>\n",
       "      <th>label</th>\n",
       "      <th>feature 5_female</th>\n",
       "      <th>feature 5_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature 1 feature 2 feature 3 feature 4 label  feature 5_female  \\\n",
       "0      29.0         0         0      29.0     1                 1   \n",
       "1    0.9167         1         2    0.9167     1                 0   \n",
       "2       2.0         1         2       2.0     0                 1   \n",
       "3      30.0         1         2      30.0     0                 0   \n",
       "4      25.0         1         2      25.0     0                 1   \n",
       "\n",
       "   feature 5_male  \n",
       "0               0  \n",
       "1               1  \n",
       "2               0  \n",
       "3               1  \n",
       "4               0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])\n",
    "\n",
    "# Print the new features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Print first five rows of df1\n",
    "display(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of characters\n",
    "\n",
    "The most basic feature we can extract from text is the **number of characters**, including whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of characters\n",
    "text = \"I don't know.\"\n",
    "num_char = len(text)\n",
    "\n",
    "# Print the number of characters\n",
    "print(num_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our DataFrame has a textual feature (say 'review'), we can compute the number of characters for each review and store it as a new feature 'num_chars' by using the pandas DataFrame **apply** method.\n",
    "\n",
    "```python\n",
    "# Create a 'num_chars' feature\n",
    "df['num_chars'] = df['review'].apply(len)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words\n",
    "\n",
    "Assuming that every word is seperated by a space, we can use a string's **split()** method to convert it into a list where every element is a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'had', 'a', 'little', 'lamb.']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Split the string into words\n",
    "text = \"Mary had a little lamb.\"\n",
    "words = text.split()\n",
    "\n",
    "# Print the list containing words\n",
    "print(words)\n",
    "\n",
    "# Print the number of words\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this for a textual feature in a DataFrame, we first define a function that takes in a string as an argument and returns the number of words in it. The steps followed inside the function are similar as before.\n",
    "\n",
    "We then pass this function **word_count** into **apply**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Function that returns number of words in a string\n",
    "def word_count(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the length of words list\n",
    "    return len(words)\n",
    "\n",
    "# Create num_words feature in df\n",
    "df['num_words'] = df['review'].apply(word_count)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word length\n",
    "\n",
    "```python\n",
    "# Function that returns average word length\n",
    "def avg_word_length(x):\n",
    "    # Split the string into words\n",
    "    words = x.split()\n",
    "    # Compute length of each word and store it in a separate list\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    # Compute average word length\n",
    "    avg_word_length = sum(word_lengths)/len(words)\n",
    "    # Return average word length\n",
    "    return(avg_word_length)\n",
    "\n",
    "# Create a new feature avg_word_length\n",
    "df['avg_word_length'] = df['review'].apply(avg_word_length)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special features\n",
    "\n",
    "When working with data such as tweets, it maybe useful to compute the number of hashtags or mentions used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of hashtags\n",
    "def hashtag_count(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    # Create a list of hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    # Return number of hashtags\n",
    "    return len(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_count(\"@janedoe This is my first tweet! #FirstTweet #Happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other features\n",
    "- Number of sentences\n",
    "- Number of paragraphs\n",
    "- Words starting with an uppercase\n",
    "- All-capital words\n",
    "- Numeric quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Character count of Russian tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.462\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('russian_tweets.csv')\n",
    "\n",
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Word count of TED talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987.1\n"
     ]
    }
   ],
   "source": [
    "ted = pd.read_csv('ted.csv')\n",
    "\n",
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Hashtags and mentions in Russian tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function that returns number of mentions in a string\n",
    "def count_mentions(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of readability tests\n",
    "- Determine **readability** of an English passage (in other words, it indicates at what educational level a person needs to be in, in order to comprehend a particular piece of text)\n",
    "- Scale ranging from primary school up to college graduate level\n",
    "- A mathematical formula utilizing word, syllable and sentence count\n",
    "- Used in fake news and opinion spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability text examples\n",
    "\n",
    "Tests that are used for texts in English:\n",
    "- **Flesch reading ease**\n",
    "- **Gunning fog index**\n",
    "- Simple Measure of Gobbledygook (SMOG)\n",
    "- Dale-Chall score\n",
    "\n",
    "Tests for other languages also exist that take into consideration the nuances of that particular language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flesch reading ease\n",
    "- One of the oldest and most widely used tests\n",
    "- Dependent on two factors:\n",
    "    - **Greater the average sentence length, harder the text is to read**\n",
    "        - \"This is a short sentence.\"\n",
    "        - \"This is a longer sentence with more words and it is harder to follow that the first sentence.\"\n",
    "    - **Greater the average number of syllables in a word, harder the text is to read**\n",
    "        - \"I live in my home.\"\n",
    "        - \"I reside in my domicile.\"\n",
    "        \n",
    "- Higher the score, greater the readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flesh reading ease score interpretation\n",
    "\n",
    "| **Reading ease score** | **Grade Level** |\n",
    "|------------------------|-----------------|\n",
    "|90-100|5|\n",
    "|80-90|6|\n",
    "|70-80|7|\n",
    "|60-70|8-9|\n",
    "|50-60|10-12|\n",
    "|30-50|College|\n",
    "|0-30|College Graduate|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gunning fog index\n",
    "- Developed in 1954\n",
    "- Also dependent on average sentence length\n",
    "- Greater the percentage of complex words, harder the text is to read\n",
    "- Higher the index, lesser the readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gunning fog index interpretation\n",
    "\n",
    "| **Fog index** | **Grade Level** |\n",
    "|---------------|-----------------|\n",
    "|17|College graduate|\n",
    "|16|College senior|\n",
    "|15|College junior|\n",
    "|14|College sophomore|\n",
    "|13|College freshman|\n",
    "|12|High school senior|\n",
    "|11|High school junior|\n",
    "|10|High school sophomore|\n",
    "|9|High school freshman|\n",
    "|8|Eighth grade|\n",
    "|7|Seventh grade|\n",
    "|6|Sixth grade|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The textatistic library\n",
    "\n",
    "We can conduct these readability tests in Python using the **Textatistic** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Flesch Reading Ease is 81.67\n",
      "The Gunning Fog Index is 7.91\n"
     ]
    }
   ],
   "source": [
    "sisyphus_essay = '\\nThe gods had condemned Sisyphus to ceaselessly rolling a rock to the top of a mountain, whence the stone would fall back of its own weight. They had thought with some reason that there is no more dreadful punishment than futile and hopeless labor. If one believes Homer, Sisyphus was the wisest and most prudent of mortals. According to another tradition, however, he was disposed to practice the profession of highwayman. I see no contradiction in this. Opinions differ as to the reasons why he became the futile laborer of the underworld. To begin with, he is accused of a certain levity in regard to the gods. He stole their secrets. Egina, the daughter of Esopus, was carried off by Jupiter. The father was shocked by that disappearance and complained to Sisyphus. He, who knew of the abduction, offered to tell about it on condition that Esopus would give water to the citadel of Corinth. To the celestial thunderbolts he preferred the benediction of water. He was punished for this in the underworld. Homer tells us also that Sisyphus had put Death in chains. Pluto could not endure the sight of his deserted, silent empire. He dispatched the god of war, who liberated Death from the hands of her conqueror. It is said that Sisyphus, being near to death, rashly wanted to test his wife\\'s love. He ordered her to cast his unburied body into the middle of the public square. Sisyphus woke up in the underworld. And there, annoyed by an obedience so contrary to human love, he obtained from Pluto permission to return to earth in order to chastise his wife. But when he had seen again the face of this world, enjoyed water and sun, warm stones and the sea, he no longer wanted to go back to the infernal darkness. Recalls, signs of anger, warnings were of no avail. Many years more he lived facing the curve of the gulf, the sparkling sea, and the smiles of earth. A decree of the gods was necessary. Mercury came and seized the impudent man by the collar and, snatching him from his joys, lead him forcibly back to the underworld, where his rock was ready for him. You have already grasped that Sisyphus is the absurd hero. He is, as much through his passions as through his torture. His scorn of the gods, his hatred of death, and his passion for life won him that unspeakable penalty in which the whole being is exerted toward accomplishing nothing. This is the price that must be paid for the passions of this earth. Nothing is told us about Sisyphus in the underworld. Myths are made for the imagination to breathe life into them. As for this myth, one sees merely the whole effort of a body straining to raise the huge stone, to roll it, and push it up a slope a hundred times over; one sees the face screwed up, the cheek tight against the stone, the shoulder bracing the clay-covered mass, the foot wedging it, the fresh start with arms outstretched, the wholly human security of two earth-clotted hands. At the very end of his long effort measured by skyless space and time without depth, the purpose is achieved. Then Sisyphus watches the stone rush down in a few moments toward tlower world whence he will have to push it up again toward the summit. He goes back down to the plain. It is during that return, that pause, that Sisyphus interests me. A face that toils so close to stones is already stone itself! I see that man going back down with a heavy yet measured step toward the torment of which he will never know the end. That hour like a breathing-space which returns as surely as his suffering, that is the hour of consciousness. At each of those moments when he leaves the heights and gradually sinks toward the lairs of the gods, he is superior to his fate. He is stronger than his rock. If this myth is tragic, that is because its hero is conscious. Where would his torture be, indeed, if at every step the hope of succeeding upheld him? The workman of today works everyday in his life at the same tasks, and his fate is no less absurd. But it is tragic only at the rare moments when it becomes conscious. Sisyphus, proletarian of the gods, powerless and rebellious, knows the whole extent of his wretched condition: it is what he thinks of during his descent. The lucidity that was to constitute his torture at the same time crowns his victory. There is no fate that can not be surmounted by scorn. If the descent is thus sometimes performed in sorrow, it can also take place in joy. This word is not too much. Again I fancy Sisyphus returning toward his rock, and the sorrow was in the beginning. When the images of earth cling too tightly to memory, when the call of happiness becomes too insistent, it happens that melancholy arises in man\\'s heart: this is the rock\\'s victory, this is the rock itself. The boundless grief is too heavy to bear. These are our nights of Gethsemane. But crushing truths perish from being acknowledged. Thus, Edipus at the outset obeys fate without knowing it. But from the moment he knows, his tragedy begins. Yet at the same moment, blind and desperate, he realizes that the only bond linking him to the world is the cool hand of a girl. Then a tremendous remark rings out: \"Despite so many ordeals, my advanced age and the nobility of my soul make me conclude that all is well.\" Sophocles\\' Edipus, like Dostoevsky\\'s Kirilov, thus gives the recipe for the absurd victory. Ancient wisdom confirms modern heroism. One does not discover the absurd without being tempted to write a manual of happiness. \"What!---by such narrow ways--?\" There is but one world, however. Happiness and the absurd are two sons of the same earth. They are inseparable. It would be a mistake to say that happiness necessarily springs from the absurd. Discovery. It happens as well that the felling of the absurd springs from happiness. \"I conclude that all is well,\" says Edipus, and that remark is sacred. It echoes in the wild and limited universe of man. It teaches that all is not, has not been, exhausted. It drives out of this world a god who had come into it with dissatisfaction and a preference for futile suffering. It makes of fate a human matter, which must be settled among men. All Sisyphus\\' silent joy is contained therein. His fate belongs to him. His rock is a thing. Likewise, the absurd man, when he contemplates his torment, silences all the idols. In the universe suddenly restored to its silence, the myriad wondering little voices of the earth rise up. Unconscious, secret calls, invitations from all the faces, they are the necessary reverse and price of victory. There is no sun without shadow, and it is essential to know the night. The absurd man says yes and his efforts will henceforth be unceasing. If there is a personal fate, there is no higher destiny, or at least there is, but one which he concludes is inevitable and despicable. For the rest, he knows himself to be the master of his days. At that subtle moment when man glances backward over his life, Sisyphus returning toward his rock, in that slight pivoting he contemplates that series of unrelated actions which become his fate, created by him, combined under his memory\\'s eye and soon sealed by his death. Thus, convinced of the wholly human origin of all that is human, a blind man eager to see who knows that the night has no end, he is still on the go. The rock is still rolling. I leave Sisyphus at the foot of the mountain! One always finds one\\'s burden again. But Sisyphus teaches the higher fidelity that negates the gods and raises rocks. He too concludes that all is well. This universe henceforth without a master seems to him neither sterile nor futile. Each atom of that stone, each mineral flake of that night filled mountain, in itself forms a world. The struggle itself toward the heights is enough to fill a man\\'s heart. One must imagine Sisyphus happy.\\n'\n",
    "\n",
    "# Import the Textatistic class\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# Create a Textatistic Object\n",
    "readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "# Generate scores\n",
    "print(f\"The Flesch Reading Ease is {readability_scores['flesch_score']:.2f}\")\n",
    "print(f\"The Gunning Fog Index is {readability_scores['gunningfog_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Readability of various publications\n",
    "\n",
    "The excerpts are available as the following strings:\n",
    "\n",
    "<code style=\"background:#RRGGBB\">forbes</code> - An excerpt from an article from Forbes magazine on the Chinese social credit score system.\n",
    "\n",
    "<code style=\"background:#RRGGBB\">harvard_law</code> - An excerpt from a book review published in Harvard Law Review.\n",
    "\n",
    "<code style=\"background:#RRGGBB\">r_digest</code> - An excerpt from a Reader's Digest article on flight turbulence.\n",
    "\n",
    "<code style=\"background:#RRGGBB\">time_kids</code> - An excerpt from an article on the ill effects of salt consumption published in TIME for Kids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbes = '\\nThe idea is to create more transparency about companies and individuals that are breaking the law or are non-compliant with official obligations and incentivize the right behaviors with the overall goal of improving governance and market order. The Chinese Communist Party intends the social credit score system to “allow the trustworthy to roam freely under heaven while making it hard for the discredited to take a single step.” Even though the system is still under development it currently plays out in real life in myriad ways for private citizens, businesses and government officials. Generally, higher credit scores give people a variety of advantages. Individuals are often given perks such as discounted energy bills and access or better visibility on dating websites. Often, those with higher social credit scores are able to forgo deposits on rental properties, bicycles, and umbrellas. They can even get better travel deals. In addition, Chinese hospitals are currently experimenting with social credit scores. A social credit score above 650 at one hospital allows an individual to see a doctor without lining up to pay.\\n'\n",
    "harvard_law = '\\nIn his important new book, The Schoolhouse Gate: Public Education, the Supreme Court, and the Battle for the American Mind, Professor Justin Driver reminds us that private controversies that arise within the confines of public schools are part of a broader historical arc — one that tracks a range of cultural and intellectual flashpoints in U.S. history. Moreover, Driver explains, these tensions are reflected in constitutional law, and indeed in the history and jurisprudence of the Supreme Court. As such, debates that arise in the context of public education are not simply about the conflict between academic freedom, public safety, and student rights. They mirror our persistent struggle to reconcile our interest in fostering a pluralistic society, rooted in the ideal of individual autonomy, with our desire to cultivate a sense of national unity and shared identity (or, put differently, our effort to reconcile our desire to forge common norms of citizenship with our fear of state indoctrination and overencroachment). In this regard, these debates reflect the unique role that both the school and the courts have played in defining and enforcing the boundaries of American citizenship. \\n'\n",
    "r_digest = '\\nThis week 30 passengers were reportedly injured when a Turkish Airlines flight landing at John F. Kennedy International Airport encountered turbulent conditions. Injuries included bruises, bloody noses, and broken bones. In mid-February, a Delta Airlines flight made an emergency landing to assist three passengers in getting to the nearest hospital after some sudden and unexpected turbulence. Doctors treated 15 passengers after a flight from Miami to Buenos Aires last October for everything from severe bruising to nosebleeds after the plane caught some rough winds over Brazil. In 2016, 23 passengers were injured on a United Airlines flight after severe turbulence threw people into the cabin ceiling. The list goes on. Turbulence has been become increasingly common, with painful outcomes for those on board. And more costly to the airlines, too. Forbes estimates that the cost of turbulence has risen to over $500 million each year in damages and delays. And there are no signs the increase in turbulence will be stopping anytime soon.\\n'\n",
    "time_kids = '\\nThat, of course, is easier said than done. The more you eat salty foods, the more you develop a taste for them. The key to changing your diet is to start small. “Small changes in sodium in foods are not usually noticed,” Quader says. Eventually, she adds, the effort will reset a kid’s taste buds so the salt cravings stop. Bridget Murphy is a dietitian at New York University’s Langone Medical Center. She suggests kids try adding spices to their food instead of salt. Eating fruits and veggies and cutting back on packaged foods will also help. Need a little inspiration? Murphy offers this tip: Focus on the immediate effects of a diet that is high in sodium. High blood pressure can make it difficult to be active. “Do you want to be able to think clearly and perform well in school?” she asks. “If you’re an athlete, do you want to run faster?” If you answered yes to these questions, then it’s time to shake the salt habit.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.436002482929858, 20.735401069518716, 11.085587583148559, 5.926785009861934]\n"
     ]
    }
   ],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "    readability_scores = Textatistic(excerpt).scores\n",
    "    gunning_fog = readability_scores['gunningfog_score']\n",
    "    gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing, POS tagging and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text sources\n",
    "- News articles\n",
    "- Tweets\n",
    "- Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making text machine friendly\n",
    "\n",
    "It is important that we standardize these texts into a machine friendly format. We want our models to treat similar words as the same.\n",
    "\n",
    "Examples:\n",
    "- <code style=\"background:#RRGGBB\">Dogs</code>, <code style=\"background:#RRGGBB\">dog</code>\n",
    "- <code style=\"background:#RRGGBB\">reduction</code>, <code style=\"background:#RRGGBB\">REDUCING</code>, <code style=\"background:#RRGGBB\">Reduce</code>\n",
    "- <code style=\"background:#RRGGBB\">don't</code>, <code style=\"background:#RRGGBB\">do not</code>\n",
    "- <code style=\"background:#RRGGBB\">won't</code>, <code style=\"background:#RRGGBB\">will not</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing techniques\n",
    "\n",
    "- Converting words into lowercase\n",
    "- Removing leading and trailing whitespaces\n",
    "- Removing punctuation\n",
    "- Removing stopwords\n",
    "- Expanding contractions\n",
    "- Removing special characters (numbers, emojis, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "**Tokenization** is the process of splitting a string into its constituent tokens.\n",
    "\n",
    "These **tokens** may be _sentences_, _words_ or _puncuations_ and is specific to a particular language.\n",
    "\n",
    "\n",
    "- Example:\n",
    "```python\n",
    "\"I have a dog. His name is Hachi.\n",
    "```\n",
    "- **Tokens:**\n",
    "```python\n",
    "[\"I\", \"have\", \"a\", \"dog\", \".\", \"His\", \"name\", \"is\", \"Hachi\", \".\"]\n",
    "```\n",
    "\n",
    "\n",
    "Tokenization also involves expanding contracted words.\n",
    "- Example:\n",
    "```python\n",
    "\"Don't do this.\"\n",
    "```\n",
    "- **Tokens**:\n",
    "```python\n",
    "[\"Do\", \"n't\", \"do\", \"this\", \".\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using spaCy\n",
    "\n",
    "To perform tokenization in python, we will use the **spaCy** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', \"'m\", 'doing', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generate list of tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "**Lemmatization** is the process of converting a words into its lowercased base form or lemma. This is an extremely powerful process of standardization.\n",
    "\n",
    "Lemmatization also allows us to convert words with apostrophes into their full forms.\n",
    "\n",
    "Examples:\n",
    "- <code style=\"background:#RRGGBB\">reducing</code>, <code style=\"background:#RRGGBB\">reduces</code>, <code style=\"background:#RRGGBB\">reduced</code>, <code style=\"background:#RRGGBB\">reduction</code> --> <code style=\"background:#RRGGBB\">reduce</code>\n",
    "- <code style=\"background:#RRGGBB\">am</code>, <code style=\"background:#RRGGBB\">are</code>, <code style=\"background:#RRGGBB\">is</code> -- > <code style=\"background:#RRGGBB\">be</code>\n",
    "- <code style=\"background:#RRGGBB\">n't</code> --> <code style=\"background:#RRGGBB\">not</code>\n",
    "- <code style=\"background:#RRGGBB\">'ve</code> --> <code style=\"background:#RRGGBB\">have</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', 'be', 'do', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generate list of lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning techniques\n",
    "- Unnecessary whitespaces and escape sequences\n",
    "- Punctuations\n",
    "- Special characters (numbers, emojis, etc.)\n",
    "- Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isalpha()\n",
    "\n",
    "Every python string has an **isalpha()** method that returns true if all the characters of the string are alphabets.\n",
    "\n",
    "This is an extremely convenient method to remove all (lemmatized) tokens that are or contain numbers, punctuations, and emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"Dog\".isalpha())\n",
    "print(\"3dogs\".isalpha())\n",
    "print(\"12347\".isalpha())\n",
    "print(\"!\".isalpha())\n",
    "print(\"?\".isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word of caution\n",
    "\n",
    "Remember that **isalpha()** has a tendency of returning false on words we would not want to remove.\n",
    "\n",
    "Examples:\n",
    "- Abbreviations: <code style=\"background:#RRGGBB\">U.S.A</code>, <code style=\"background:#RRGGBB\">U.K.</code>, etc.\n",
    "- Proper Nouns: <code style=\"background:#RRGGBB\">word2vec</code> and <code style=\"background:#RRGGBB\">xto10x</code>\n",
    "\n",
    "For such nuanced cases, **isalpha()** may not be sufficient. It may be advisable to write your own custom function, typically using regex, to ensure you're not inadvertently removing useful words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG this be like the good thing ever wow such an amazing song I be hooked top definitely\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\"\n",
    "OMG!!!! This is like    the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Generate a list of tokens\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove tokens that are not alphabetic\n",
    "a_lemmas = [lemma for lemma in lemmas\n",
    "            if lemma.isalpha() or lemma == '-PRON-']\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "- Words that occur extremely commonly\n",
    "- Eg. **articles** (e.g. a, the), **be verbs** (e.g. is, am), **pronouns** (e.g. he, she), etc.\n",
    "\n",
    "Always exercise caution while using third party stopword lists. It is common that an application find certain words useful that may be considered a stopword by third party lists. It is often advisable to create your own custom stopword lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG like good thing wow amazing song I hooked definitely\n"
     ]
    }
   ],
   "source": [
    "# Get list of stopwords\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "string = \"\"\"\n",
    "OMG!!!! This is like    the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
    "\"\"\"\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas\n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other text preprocessing techniques\n",
    "- Removing HTML/XML tags\n",
    "- Replacing accented characters (such as é)\n",
    "- Correcting spelling errors and shorthands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word of caution\n",
    "\n",
    "The text preprocessing techniques you use is always dependent on the application. There are many applications which may find punctuations, numbers, and emojis useful, so it may be wise to not remove them.\n",
    "\n",
    "Always use only those techniques that are relevant to your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Cleaning TED talks in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['fifteen', 'noone', 'whereupon', 'could', 'ten', 'all', 'please', 'indeed', 'whole', 'beside', 'therein', 'using',\n",
    "             'but', 'very', 'already', 'about', 'no', 'regarding', 'afterwards', 'front', 'go', 'in', 'make', 'three', 'here',\n",
    "             'what', 'without', 'yourselves', 'which', 'nothing', 'am', 'between', 'along', 'herein', 'sometimes', 'did', 'as',\n",
    "             'within', 'elsewhere', 'was', 'forty', 'becoming', 'how', 'will', 'other', 'bottom', 'these', 'amount', 'across',\n",
    "             'the', 'than', 'first', 'namely', 'may', 'none', 'anyway', 'again', 'eleven', 'his', 'meanwhile', 'name', 're',\n",
    "             'from', 'some', 'thru', 'upon', 'whither', 'he', 'such', 'down', 'my', 'often', 'whether', 'made', 'while',\n",
    "             'empty', 'two', 'latter', 'whatever', 'cannot', 'less', 'many', 'you', 'ours', 'done', 'thus', 'since',\n",
    "             'everything', 'for', 'more', 'unless', 'former', 'anyone', 'per', 'seeming', 'hereafter', 'on', 'yours', 'always',\n",
    "             'due', 'last', 'alone', 'one', 'something', 'twenty', 'until', 'latterly', 'seems', 'were', 'where', 'eight',\n",
    "             'ourselves', 'further', 'themselves', 'therefore', 'they', 'whenever', 'after', 'among', 'when', 'at', 'through',\n",
    "             'put', 'thereby', 'then', 'should', 'formerly', 'third', 'who', 'this', 'neither', 'others', 'twelve', 'also',\n",
    "             'else', 'seemed', 'has', 'ever', 'someone', 'its', 'that', 'does', 'sixty', 'why', 'do', 'whereas', 'are',\n",
    "             'either', 'hereupon', 'rather', 'because', 'might', 'those', 'via', 'hence', 'itself', 'show', 'perhaps',\n",
    "             'various', 'during', 'otherwise', 'thereafter', 'yourself', 'become', 'now', 'same', 'enough', 'been', 'take',\n",
    "             'their', 'seem', 'there', 'next', 'above', 'mostly', 'once', 'a', 'top', 'almost', 'six', 'every', 'nobody',\n",
    "             'any', 'say', 'each', 'them', 'must', 'she', 'throughout', 'whence', 'hundred', 'not', 'however', 'together',\n",
    "             'several', 'myself', 'i', 'anything', 'somehow', 'or', 'used', 'keep', 'much', 'thereupon', 'ca', 'just',\n",
    "             'behind', 'can', 'becomes', 'me', 'had', 'only', 'back', 'four', 'somewhere', 'if', 'by', 'whereafter',\n",
    "             'everywhere', 'beforehand', 'well', 'doing', 'everyone', 'nor', 'five', 'wherein', 'so', 'amongst', 'though',\n",
    "             'still', 'move', 'except', 'see', 'us', 'your', 'against', 'although', 'is', 'became', 'call', 'have', 'most',\n",
    "             'wherever', 'few', 'out', 'whom', 'yet', 'be', 'own', 'off', 'quite', 'with', 'and', 'side', 'whoever', 'would',\n",
    "             'both', 'fifty', 'before', 'full', 'get', 'sometime', 'beyond', 'part', 'least', 'besides', 'around', 'even',\n",
    "             'whose', 'hereby', 'up', 'being', 'we', 'an', 'him', 'below', 'moreover', 'really', 'it', 'of', 'our', 'nowhere',\n",
    "             'whereby', 'too', 'her', 'toward', 'anyhow', 'give', 'never', 'another', 'anywhere', 'mine', 'herself', 'over',\n",
    "             'himself', 'to', 'onto', 'into', 'thence', 'towards', 'hers', 'nevertheless', 'serious', 'under', 'nine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     talk new lecture TED I illusion create TED I t...\n",
      "1     representation brain brain break left half log...\n",
      "2     great honor today share Digital Universe creat...\n",
      "3     passion music technology thing combination thi...\n",
      "4     use want computer new program programming requ...\n",
      "5     I neuroscientist mixed background physics medi...\n",
      "6     Pat Mitchell day January begin like work love ...\n",
      "7     Taylor Wilson I year old I nuclear physicist l...\n",
      "8     I grow Northern Ireland right north end absolu...\n",
      "9     I publish article New York Times Modern Love c...\n",
      "10    Joseph Member Parliament Kenya picture Maasai ...\n",
      "11    hi I talk little bit music machine life specif...\n",
      "12    hi let I ask audience question lie child raise...\n",
      "13    historical record allow know ancient Greeks dr...\n",
      "14    good morning I little boy I experience change ...\n",
      "15    I slide I year ago time I short slide morning ...\n",
      "16    I like world I like share year old love story ...\n",
      "17    I fail woman I fail feminist I passionate opin...\n",
      "18    revolution century significant longevity revol...\n",
      "19    today baffled lady observe shell soul dwellsan...\n",
      "Name: transcript, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ted = pd.read_csv('ted.csv', nrows=20)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "    # Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "**Part-of-speech tagging** (**POS tagging**) has an immense number of applications in NLP.\n",
    "- Word sense disambiguation\n",
    "    - <code style=\"background:#RRGGBB\">\"The bear is a majestic animal\"</code>\n",
    "    - <code style=\"background:#RRGGBB\">\"Please bear with me\"</code>\n",
    "- Sentiment analysis\n",
    "- Question answering\n",
    "- Fake news and opinion spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "**POS tagging** is the process of assigning every word (or token) in a piece of text, its corresponding part-of-speech.\n",
    "\n",
    "- Example:\n",
    "```python\n",
    "\"Jane is an amazing guitarist.\"\n",
    "```\n",
    "- **POS Tagging:**\n",
    "    - <code style=\"background:#RRGGBB\">Jane</code> --> **proper noun**\n",
    "    - <code style=\"background:#RRGGBB\">is</code> --> **verb**\n",
    "    - <code style=\"background:#RRGGBB\">an</code> --> **determiner**\n",
    "    - <code style=\"background:#RRGGBB\">amazing</code> --> **adjective**\n",
    "    - <code style=\"background:#RRGGBB\">guitaris</code> --> **noun**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jane', 'PROPN'), ('is', 'AUX'), ('an', 'DET'), ('amazing', 'ADJ'), ('guitarist', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Generate a list of tokens\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize string\n",
    "string = \"Jane is an amazing guitarist\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generating list of tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that POS tagging is not an exact science. spaCy infers the POS tags of these words based on the predictions given by its pre-trained models.\n",
    "\n",
    "In other words, the accuracy of the POS tagging is dependent on the data that the model has been trained on and the data that it is being used on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS annotations in spaCy\n",
    "\n",
    "spaCy is capable of identifying close to 20 parts-of-speech and it uses specific annotations to denote a particular part-of-speech.\n",
    "\n",
    "- <code style=\"background:#RRGGBB\">PROPN</code> --> proper noun\n",
    "- <code style=\"background:#RRGGBB\">DET</code> --> determinant\n",
    "- spaCy annotations at https://spacy.io/api/annotation\n",
    "\n",
    "| **POS** | **DESCRIPTION** | **EXAMPLES** |\n",
    "|---------|-----------------|--------------|\n",
    "|<code style=\"background:#RRGGBB\">ADJ</code>|adjective|big, old, green, incomprehensible, first|\n",
    "|<code style=\"background:#RRGGBB\">ADP</code>|adposition|in, to, during|\n",
    "|<code style=\"background:#RRGGBB\">ADV</code>|adverb|very, tomorrow, down, where, there|\n",
    "|<code style=\"background:#RRGGBB\">AUX</code>|auxiliary|is, has (done), will (do), should (do)|\n",
    "|<code style=\"background:#RRGGBB\">CONJ</code>|conjunction|and, or, but|\n",
    "|<code style=\"background:#RRGGBB\">CCONJ</code>|coordinating conjunction|and, or, but|\n",
    "|<code style=\"background:#RRGGBB\">DET</code>|determiner|a, an, the|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Counting nouns in a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Noun usage in fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of proper nouns in real and fake headlines are 2.46 and 4.86 respectively\n"
     ]
    }
   ],
   "source": [
    "headlines = pd.read_csv('fakenews.csv')\n",
    "\n",
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of other nouns in real and fake headlines are 2.21 and 1.60 respectively\n"
     ]
    }
   ],
   "source": [
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "**Named entity recognition** (**NER**) has a host of extremely useful applications.\n",
    "\n",
    "- Efficient search algorithms\n",
    "- Question answering\n",
    "- News article classification\n",
    "- Customer service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "A **named entity** is anything that can be denoted with a proper name or a proper noun.\n",
    "\n",
    "**Named entity recognition** (**NER**), therefore, is the process of identifying such named entities in a piece of text and classifying them into predefined categories such as _person_, _organization_, _country_, etc.\n",
    "\n",
    "- Example:\n",
    "```python\n",
    "\"John Doe is a software engineer working at Google. He lives in France.\"\n",
    "```\n",
    "- **POS Tagging:**\n",
    "    - <code style=\"background:#RRGGBB\">John Doe</code> --> **person**\n",
    "    - <code style=\"background:#RRGGBB\">Google</code> --> **organization**\n",
    "    - <code style=\"background:#RRGGBB\">France</code> --> **country** (**geopolitical entity**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John Doe', 'PERSON'), ('France', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "string = \"John Doe is a software engineer working at Google. He lives in France.\"\n",
    "\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generate named entities\n",
    "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER annotations in spaCy\n",
    "\n",
    "- More than 15 categories of named entities\n",
    "- NER annotations at https://spacy.io/api/annotation#named-entities\n",
    "\n",
    "| **TYPE** | **DESCRIPTION** |\n",
    "|---------|-----------------|\n",
    "|<code style=\"background:#RRGGBB\">PERSON</code>|People, including fictional.|\n",
    "|<code style=\"background:#RRGGBB\">NORP</code>|Nationalities or religious or political groups.|\n",
    "|<code style=\"background:#RRGGBB\">FAC</code>|Buildings, airports, highways, bridges, etc.|\n",
    "|<code style=\"background:#RRGGBB\">ORG</code>|Companies, agencies, institutions, etc.|\n",
    "|<code style=\"background:#RRGGBB\">GPE</code>|Countries, cities, states.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word of caution\n",
    "\n",
    "Remember that spaCy's models are not perfect and its performance depends on the data it was trained with and the data it is being used on.\n",
    "\n",
    "For instance, if you are trying to extract named entities for texts from a heavily technical field, such as medicine, spaCy's pretrained models may not perform such a great job. In such nuanced cases, it is better to train your own models with your specialized data.\n",
    "\n",
    "Also, remember that spaCy's models are language specific. This is understandable considering that each language has its own grammar and nuances. The **en_core_web_sm** model that we're using is, as the name suggests, only suitable for English texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Identifying people mentioned in a news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sheryl Sandberg', 'Mark Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "tc = \"\\nIt’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\\n\"\n",
    "\n",
    "def find_persons(text):\n",
    "    # Create Doc object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Identify the persons\n",
    "    persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "    # Return persons\n",
    "    return persons\n",
    "\n",
    "print(find_persons(tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a bag of words model\n",
    "\n",
    "**Vectorization** is the process of converting text into vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of data format for ML algorithms\n",
    "For any ML algorithm,\n",
    "- Data must be in tabular form\n",
    "- Training features must be numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words model\n",
    "- Extract word tokens\n",
    "- Compute frequency of word tokens\n",
    "- Construct a word vector out of these frequencies and vocabulary of corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words model example\n",
    "**Corpus**\n",
    "\n",
    "A corpus of three documents:\n",
    "```python\n",
    "\"The lion is the king of the jungle\"\n",
    "\"Lions have lifespans of a decade\"\n",
    "\"The lion is an endangered species\"\n",
    "```\n",
    "\n",
    "We now extract the unique word tokens that occur in this corpus of documents. This will be the **vocabulary** of our model.\n",
    "\n",
    "**Vocabulary** --> <code style=\"background:#RRGGBB\">a</code>, <code style=\"background:#RRGGBB\">an</code>, <code style=\"background:#RRGGBB\">decade</code>, <code style=\"background:#RRGGBB\">endangered</code>, <code style=\"background:#RRGGBB\">have</code>, <code style=\"background:#RRGGBB\">is</code>, <code style=\"background:#RRGGBB\">jungle</code>, <code style=\"background:#RRGGBB\">king</code>, <code style=\"background:#RRGGBB\">lifespans</code>, <code style=\"background:#RRGGBB\">lion</code>, <code style=\"background:#RRGGBB\">Lions</code>, <code style=\"background:#RRGGBB\">of</code>, <code style=\"background:#RRGGBB\">species</code>, <code style=\"background:#RRGGBB\">the</code>, <code style=\"background:#RRGGBB\">The</code>\n",
    "\n",
    "Since there are 15 words in our vocabulary, our **word vectors** will have 15 dimensions and each dimension's value will correspond to the frequency of the **word token** corresponding to that dimension.\n",
    "\n",
    "```python\n",
    "\"The lion is the king of the jungle\"\n",
    "[0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1]\n",
    "```\n",
    "\n",
    "```python\n",
    "\"Lions have lifespans of a decade\"\n",
    "[1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n",
    "```\n",
    "\n",
    "```python\n",
    "\"The lion is an endangered species\"\n",
    "[0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "- <code style=\"background:#RRGGBB\">Lions</code>, <code style=\"background:#RRGGBB\">lion</code> --> <code style=\"background:#RRGGBB\">lion</code>\n",
    "- <code style=\"background:#RRGGBB\">The</code>, <code style=\"background:#RRGGBB\">the</code> --> <code style=\"background:#RRGGBB\">the</code>\n",
    "- No punctuations\n",
    "- No stopwords\n",
    "- Leads to smaller vocabularies\n",
    "- Reducing number of dimensions helps improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words model using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 1 0 1 0 1 0 3]\n",
      " [0 1 0 1 0 0 0 1 0 1 1 0 0]\n",
      " [1 0 1 0 1 0 0 0 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.Series([\n",
    "    \"The lion is the king of the jungle\",\n",
    "    \"Lions have lifespans of a decade\",\n",
    "    \"The lion is an endangered species\"\n",
    "])\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "print(bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this is different from the word vectors we generated. This is because **CountVectorizer** automatically lowercases words and ignores single character tokens such as 'a'. Also, it doesn't necessarily index the vocabulary in alphabetical order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: BoW model for movie taglines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7033, 6614)\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv('movie_overviews.csv')['tagline'].dropna()\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Mapping feature indices with feature names\n",
    "We had seen that <code style=\"background:#RRGGBB\">CountVectorizer</code> doesn't necessarily index the vocabulary in alphabetical order. In this exercise, we will learn to map each feature index to its corresponding feature name from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>decade</th>\n",
       "      <th>endangered</th>\n",
       "      <th>have</th>\n",
       "      <th>is</th>\n",
       "      <th>jungle</th>\n",
       "      <th>king</th>\n",
       "      <th>lifespans</th>\n",
       "      <th>lion</th>\n",
       "      <th>lions</th>\n",
       "      <th>of</th>\n",
       "      <th>species</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  decade  endangered  have  is  jungle  king  lifespans  lion  lions  of  \\\n",
       "0   0       0           0     0   1       1     1          0     1      0   1   \n",
       "1   0       1           0     1   0       0     0          1     0      1   1   \n",
       "2   1       0           1     0   1       0     0          0     1      0   0   \n",
       "\n",
       "   species  the  \n",
       "0        0    3  \n",
       "1        0    0  \n",
       "2        1    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = ['The lion is the king of the jungle',\n",
    "          'Lions have lifespans of a decade',\n",
    "          'The lion is an endangered species']\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "display(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a BoW Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam filtering\n",
    "\n",
    "|**message**|**label**|\n",
    "|-----------|---------|\n",
    "|WINNER!! As a valued network customer you have been selected to receive a $900 prize reward! To claim call 09061701461| spam |\n",
    "|Ah, work. I vaguely remember that. What does it feel like?| ham |\n",
    "\n",
    "Our task is to train an ML model that can predict the label given a particular text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. Text preprocessing\n",
    "2. Building a bag-of-words model (or representation)\n",
    "3. Machine learning\n",
    "\n",
    "Note that although we use the term '**modeling**' in the context of both BoW and machine learning, they mean two different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing using CountVectorizer\n",
    "\n",
    "**CountVectorizer** arguments\n",
    "- <code style=\"background:#RRGGBB\">lowercase</code>: <code style=\"background:#RRGGBB\">False</code>, <code style=\"background:#RRGGBB\">True</code>\n",
    "- <code style=\"background:#RRGGBB\">strip_accents</code>: <code style=\"background:#RRGGBB\">'unicode'</code>, <code style=\"background:#RRGGBB\">'ascii'</code>, <code style=\"background:#RRGGBB\">None</code>\n",
    "- <code style=\"background:#RRGGBB\">stop_words</code>: <code style=\"background:#RRGGBB\">'english'</code>, <code style=\"background:#RRGGBB\">list</code>, <code style=\"background:#RRGGBB\">None</code>\n",
    "- <code style=\"background:#RRGGBB\">token_pattern</code>: <code style=\"background:#RRGGBB\">regex</code>\n",
    "- <code style=\"background:#RRGGBB\">tokenizer</code>: <code style=\"background:#RRGGBB\">function</code> (you can pass a function that takes a string as an argument and returns a list of tokens)\n",
    "\n",
    "**CountVectorizer** cannot perform certain steps such as **lemmatization** automatically. This is where **spaCy** is useful.\n",
    "\n",
    "Although it performs tokenization and preprocessing, **CountVectorizer**'s main job is to convert a corpus into a matrix of numerical vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the BoW model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', lowercase=False)\n",
    "```\n",
    "\n",
    "We set the lowercase argument to False. This is because spam messages usually tend to abuse all-capital words and we might want to preserve this information for the ML step.\n",
    "\n",
    "```python\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.25)\n",
    "\n",
    "# Generate training BoW vectors\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Generate test BoW vectors\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "```\n",
    "\n",
    "Note that we do not fit the vectorizer with the test data. It is possible that there are some words in the test data that is not in the vocabulary of the vectorizer. In such cases, **CountVectorizer** simply ignores these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Naive Bayes classifier\n",
    "\n",
    "We're now in a good position to train an ML model. We will use the Multinomial Naive Bayes classifier for this task.\n",
    "\n",
    "```python\n",
    "# Import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train clf\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Compute accuracy on test set\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: BoW vectors for movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 14898)\n",
      "(250, 14898)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('movie_reviews_clean.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'])\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Predicting the sentiment of a movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.812\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "# Import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building n-gram models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW shortcomings\n",
    "\n",
    "|**review**|**label**|\n",
    "|-----------|---------|\n",
    "|'The movie was good and not boring'| positive |\n",
    "|'The movie was not good and boring'| negative |\n",
    "\n",
    "If we were to construct BoW vectors for these reviews, we would get identical vectors since both reviews contain exactly the same words. And herein lies the biggest shortcoming of the bag-of-words model: **context of the words is lost**\n",
    "\n",
    "In this example, the position of the word 'not' changes the entire sentiment of the review. Therefore, we will study techniques that will allow us to model this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "\n",
    "An **n-gram** is a contiguous sequence of n elements (or words) in a given document.\n",
    "\n",
    "- n = 1 --> bag-of-words\n",
    "```python\n",
    "'for you a thousand times over'\n",
    "```\n",
    "- n = 2, n-grams (**bigrams**):\n",
    "```python\n",
    "[\n",
    "    'for you',\n",
    "    'you a',\n",
    "    'a thousand',\n",
    "    'thousand times',\n",
    "    'times over'\n",
    "]\n",
    "```\n",
    "- n = 3, n-grams (**trigrams**)\n",
    "```python\n",
    "[\n",
    "    'for you a',\n",
    "    'you a thousand',\n",
    "    'a thousand times',\n",
    "    'thousand times over'\n",
    "]\n",
    "    \n",
    "```\n",
    "\n",
    "Therefore, we can use these n-grams to capture more context and account for cases like 'not'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "- Sentence completion\n",
    "- Spelling correction\n",
    "- Machine translation correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building n-gram models using scikit-learn\n",
    "\n",
    "**CountVectorizer** takes in an argument **ngram_range**, which is a tuple containing the lower and upper bound for the range of n-gram values.\n",
    "\n",
    "- Generates only bigrams:\n",
    "```python\n",
    "bigrams = CountVectorizer(ngram_range=(2, 2))\n",
    "```\n",
    "\n",
    "- Generates unigrams, bigrams and trigrams:\n",
    "```python\n",
    "ngrams = CountVectorizer(ngram_range=(1, 3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcomings\n",
    "\n",
    "While on the surface, it may seem lucrative to generate n-grams of high orders to capture more and more context, it comes with caveats.\n",
    "\n",
    "We've already seen that the BoW vectors run into thousands of dimensions. Adding higher order n-grams increases the number of dimensions even more and while performing machine learning, leads to a problem known as the **curse of dimensionality**.\n",
    "\n",
    "Additionally, n-grams for n greater than 3 become exceedingly rare to find in multiple documents. So that feature becomes effectively useless.\n",
    "\n",
    "For these reasons, it is often a good idea to restrict yourself to n-grams where n is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: n-gram models for movie tag lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng1, ng2 and ng3 have 6614, 37100 and 76881 features respectively\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv('movie_overviews.csv')['tagline'].dropna()\n",
    "\n",
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Higher order n-grams for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.822\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('movie_reviews_clean.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.5)\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "ng_vectorizer = CountVectorizer(ngram_range=(1, 2), lowercase=True, stop_words='english')\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_ng = ng_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_ng = ng_vectorizer.transform(X_test)\n",
    "\n",
    "# Define an instance of MultinomialNB \n",
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# Fit the classifier \n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy \n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Comparing performance of n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 0.324 seconds to complete. The accuracy on the test set is 0.75. The ngram representation had 12347 features.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('movie_reviews_clean.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 1.768 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 178240 features.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building tf-idf document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram modeling\n",
    "- Weight of dimension dependent on the frequency of the word corresponding to the dimension.\n",
    "    - Document contains the word <code style=\"background:#RRGGBB\">human</code> in five places.\n",
    "    - Dimension corresponding to <code style=\"background:#RRGGBB\">human</code> has weight <code style=\"background:#RRGGBB\">5</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "- Some words occur very commonly across all documents.\n",
    "- Corpus of documents on the universe\n",
    "    - One document has <code style=\"background:#RRGGBB\">jupiter</code> and <code style=\"background:#RRGGBB\">universe</code> occurring 20 times each.\n",
    "    - <code style=\"background:#RRGGBB\">jupiter</code> rarely occurs in the other documents. <code style=\"background:#RRGGBB\">universe</code> is common.\n",
    "    - Give more weight to <code style=\"background:#RRGGBB\">jupiter</code> on account of exclusivity. In other words, the words <code style=\"background:#RRGGBB\">jupiter</code> characterizes the document more than <code style=\"background:#RRGGBB\">universe</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "- Automatically detect stopwords\n",
    "- Search\n",
    "- Recommender systems\n",
    "- Better performance in predictive modeling for some cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency-inverse document frequency\n",
    "- Proportional to term frequency\n",
    "- Inverse function of the number of documents in which it occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical formula\n",
    "\n",
    "<font size=\"4\">_w_<sub>_i,j_</sub> = _tf_<sub>_i,j_</sub>._log_$(\\frac{N}{df_{i}})$</font>\n",
    "\n",
    "- _w_<sub>_i,j_</sub> --> weight of term _i_ in document _j_\n",
    "- _tf_<sub>_i,j_</sub> --> term frequency of term _i_ in document _j_\n",
    "- _N_ --> number of documents in the corpus\n",
    "- _df_<sub>_i_</sub> --> number of documents containing term _i_\n",
    "\n",
    "Example:\n",
    "- Let's say the word 'library' occurs in a document 5 times.\n",
    "- There are 20 documents in the corpus and 'library' occurs in 8 of them.\n",
    "- Then, the tf-idf weight of 'library' in the vector representation of this document will be:\n",
    "<font size=\"3\">_w_<sub>_library,document_</sub> = 5._log_$(\\frac{20}{8})$ ≈ 2</font>\n",
    "\n",
    "In general, higher the tf-idf weight, more important is the word in characterizing the document. A high tf-idf for a word in a document may imply that the word is relatively exclusive to that particular document or that the word occurs extremely commonly in the document, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "print(tfidf_matrix.toarray())\n",
    "```\n",
    "\n",
    "The parameters and methods **TfidfVectorizer** has is almost identical to **CountVectorizer**. The only difference is that **TfidfVectorizer** assigns weights using the tf-idf formula and has extra parameters related to inverse document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: tf-idf vectors for TED talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 29158)\n"
     ]
    }
   ],
   "source": [
    "ted = pd.read_csv('ted.csv')['transcript']\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "The **cosine similarity** score of two vectors is the cosine of the angle between the vectors. Mathematically, it is the ratio of the dot product of the vectors and the product of the magnitude of the two vectors.\n",
    "\n",
    "<font size=\"4\">_sim_(_A_, _B_) = cos($\\theta$) = $\\frac{A.B}{||A||||B||}$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dot product\n",
    "\n",
    "The **dot product** is computed by summing the product of values across corresponding dimensions of the vectors.\n",
    "\n",
    "Consider two vectors,\n",
    "\n",
    "_V_ = (_v_<sub>1</sub>, _v_<sub>2</sub>,...,_v_<sub>n</sub>), _W_ = (_w_<sub>1</sub>, _w_<sub>2</sub>,..., _w_<sub>n</sub>)\n",
    "\n",
    "Then the dot product of V and W is,\n",
    "\n",
    "_V_._W_ = (_v_<sub>1</sub> x _w_<sub>1</sub>) + (_v_<sub>2</sub> x _w_<sub>2</sub>) + ... + (_v_<sub>n</sub> x _w_<sub>n</sub>)\n",
    "\n",
    "Example:\n",
    "\n",
    "_A_ = (4, 7, 1), _B_ = (5, 2, 3)\n",
    "\n",
    "_A_._B_ = (4 x 5) + (7 x 2) + (1 x 3) = 20 + 14 + 3 = 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude of a vector\n",
    "\n",
    "The **magnitude** of a vector is essentially the length of the vector. Mathematically, it is defined as the square root of the sum of the squares of values across all the dimensions of a vector.\n",
    "\n",
    "For any vector,\n",
    "\n",
    "_V_ = (_v_<sub>1</sub>, _v_<sub>2</sub>,...,_v_<sub>n</sub>)\n",
    "\n",
    "The magnitude is defined as,\n",
    "||**V**|| = $\\sqrt{(v_{1})^{2} + (v_{2})^{2} + ... + (v_{n})^{2}}$\n",
    "\n",
    "Example:\n",
    "_A_ = (4, 7, 1), _B_ = (5, 2, 3)\n",
    "\n",
    "||**A**|| = $\\sqrt{(4)^{2} + (7)^{2} + (1)^{2}}$ = $\\sqrt{16 + 49 + 1}$ = $\\sqrt{66}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cosine score\n",
    "\n",
    "_A_: (4, 7, 1)\n",
    "_B_: (5, 2, 3)\n",
    "\n",
    "The cosine score,\n",
    "\n",
    "<font size=\"4\">cos(_A,B_) = $\\frac{A.B}{|A|.|B|}$ = $\\frac{37}{\\sqrt{66} x \\sqrt{38}}$ = 0.7388</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Score: points to remember\n",
    "\n",
    "Since the cosine score is simply the cosine of the angle between two vectors, its value is bounded between -1 and 1.\n",
    "\n",
    "However, in NLP, document vectors almost always use non-negative weights. Therefore, cosine scores vary between 0 and 1, weher 0 indicates no similarity and 1 indicates that the documents are identical.\n",
    "\n",
    "Finally, since the cosine score ignores the magnitude of the vectors, it is fairly robust to document length. This may be an advantage or a disadvantage depending on the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation using scikit-learn\n",
    "\n",
    "**scikit-learn** offers a **cosine_similarity** function that outputs a similarity matrix containing the pairwise cosine scores for a set of vectors.\n",
    "\n",
    "However, remember that **cosine_similarity** takes in 2-D arrays as arguments. Passing in 1-D arrays will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73881883]]\n"
     ]
    }
   ],
   "source": [
    "# Import the cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define two 3-dimensional vectors A and B\n",
    "A = (4, 7, 1)\n",
    "B = (5, 2, 3)\n",
    "\n",
    "# Compute the cosine score of A and B\n",
    "score = cosine_similarity([A], [B])\n",
    "\n",
    "# Print the cosine score\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Cosine similarity matrix of a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "corpus = ['The sun is the largest celestial body in the solar system',\n",
    "          'The solar system consists of the sun and eight revolving planets',\n",
    "          'Ra was the Egyptian Sun God',\n",
    "          'The Pyramids were the pinnacle of Egyptian architecture',\n",
    "          'The quick brown fox jumps over the lazy dog']\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import the cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a plot line based recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie recommender\n",
    "\n",
    "|**Title**|**Overview**|\n",
    "|---------|------------|\n",
    "|Shanghai Triad|A provincial boy related to a Shanghai crime family is recruited by his uncle into cosmopolitan Shanghai in the 1930s to be a servant to a ganglord's mistress.|\n",
    "|Cry, the Beloved Country|A South-African preacher goes to search for his wayward son who has committed a crime in the big city.|\n",
    "\n",
    "Our task is to build a system that takes in a movie title and outputs a list of movies that has similar plot lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. Text preprocessing\n",
    "2. Generate tf-idf vectors\n",
    "3. Generate cosine similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The recommender function\n",
    "1. Take a movie title, cosine similarity matrix and indices series as arguments. (The **indices series** is a reverse mapping of movie titles with their indices in the original dataframe)\n",
    "2. Extract pairwise cosine similarity scores for the movie.\n",
    "3. Sort the scores in descending order.\n",
    "4. Output titles corresponding to the highest scores.\n",
    "5. Ignore the highest similarity score (of 1). This is because the movie most similar to a given movie is the movie itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating tf-idf vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Generate cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "```\n",
    "\n",
    "This generate a matrix that contains the pairwise similarity score of every movie with every other movie. The value corresponding to the ith row and jth column is the cosine similarity score of movie i with movie j.\n",
    "\n",
    "Notice that the diagonal elements of this matrix is 1. This is because the cosine similarity score of movie k with itself is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linear_kernel function\n",
    "\n",
    "- Magnitude of a tf-idf vector is 1\n",
    "- Cosine score between two tf-idf vectors is their dot product.\n",
    "- Can significantly improve computation time.\n",
    "- Use <code style=\"background:#RRGGBB\">linear_kernel</code> instead of <code style=\"background:#RRGGBB\">cosine_similarity</code>. <code style=\"background:#RRGGBB\">linear_kernel</code> function computes the pairwise dot product of every vector with every other vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating cosine similarity matrix\n",
    "\n",
    "```python\n",
    "# Import linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Generate cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "```\n",
    "\n",
    "The output remains the same but it takes significantly lesser time to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Comparing linear_kernel and cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "import time\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(pd.read_csv('movie_overviews.csv')['overview'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.01801346 0.         ... 0.         0.         0.01172837]\n",
      " [0.01801346 1.         0.04893151 ... 0.         0.0061698  0.0116183 ]\n",
      " [0.         0.04893151 1.         ... 0.         0.0077194  0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.         0.        ]\n",
      " [0.         0.0061698  0.0077194  ... 0.         1.         0.00489639]\n",
      " [0.01172837 0.0116183  0.         ... 0.         0.00489639 1.        ]]\n",
      "Time taken: 1.060391902923584 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.01801346 0.         ... 0.         0.         0.01172837]\n",
      " [0.01801346 1.         0.04893151 ... 0.         0.0061698  0.0116183 ]\n",
      " [0.         0.04893151 1.         ... 0.         0.0077194  0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.         0.        ]\n",
      " [0.         0.0061698  0.0077194  ... 0.         1.         0.00489639]\n",
      " [0.01172837 0.0116183  0.         ... 0.         0.00489639 1.        ]]\n",
      "Time taken: 3.2841598987579346 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond n-grams: word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem with BoW and tf-idf\n",
    "\n",
    "Consider the three sentences:\n",
    "```python\n",
    "'I am happy'\n",
    "'I am joyous'\n",
    "'I am sad'\n",
    "```\n",
    "\n",
    "Now, if we were to compute the similarities, 'I am happy' and 'I am joyous' will have the same score as 'I am happy' and 'I am sad', regardless of how we vectorize it. This is because 'happy', 'joyous' and 'sad' are considered to be completely different words. However, we know that 'happy' and 'joyous' are more similar to each other than 'sad'. This is something that the vectorization techniques we've covered so far simply cannot capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "\n",
    "**Word embeddings** is the process of mapping words into an n-dimensional vector space. These vectors are usually produced using deep learning models and huge amounts of data.\n",
    "\n",
    "These vectors can be used to discern how similar two words are to each other. Consequently, they can also be used to detect synonyms and antonyms.\n",
    "\n",
    "**Word embeddings** are also capable of capturing complex relationships.\n",
    "- For instance, it can be used to detect that the words <code style=\"background:#RRGGBB\">King</code> and <code style=\"background:#RRGGBB\">Queen</code> relate to each other the same way as <code style=\"background:#RRGGBB\">Man</code> and <code style=\"background:#RRGGBB\">Woman</code>.\n",
    "- Or that <code style=\"background:#RRGGBB\">France</code> and <code style=\"background:#RRGGBB\">Paris</code> are related in the same way as <code style=\"background:#RRGGBB\">Russia</code> and <code style=\"background:#RRGGBB\">Moscow</code>.\n",
    "\n",
    "One last things to note is that word embeddings are not trained on user data; they are dependent on the pre-trained **spaCy** model you're using and are independent of the size of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings using spaCy\n",
    "\n",
    "Note that it is advisable to load larger **spaCy** models while working with **word vectors**. This is because the '**en_core_web_sm**' model does not technically ship with word vectors but context specific tensors, which tend to give relatively poor results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.8733e-01  4.0595e-01 -5.1174e-01 -5.5482e-01  3.9716e-02  1.2887e-01\n",
      "  4.5137e-01 -5.9149e-01  1.5591e-01  1.5137e+00 -8.7020e-01  5.0672e-02\n",
      "  1.5211e-01 -1.9183e-01  1.1181e-01  1.2131e-01 -2.7212e-01  1.6203e+00\n",
      " -2.4884e-01  1.4060e-01  3.3099e-01 -1.8061e-02  1.5244e-01 -2.6943e-01\n",
      " -2.7833e-01 -5.2123e-02 -4.8149e-01 -5.1839e-01  8.6262e-02  3.0818e-02\n",
      " -2.1253e-01 -1.1378e-01 -2.2384e-01  1.8262e-01 -3.4541e-01  8.2611e-02\n",
      "  1.0024e-01 -7.9550e-02 -8.1721e-01  6.5621e-03  8.0134e-02 -3.9976e-01\n",
      " -6.3131e-02  3.2260e-01 -3.1625e-02  4.3056e-01 -2.7270e-01 -7.6020e-02\n",
      "  1.0293e-01 -8.8653e-02 -2.9087e-01 -4.7214e-02  4.6036e-02 -1.7788e-02\n",
      "  6.4990e-02  8.8451e-02 -3.1574e-01 -5.8522e-01  2.2295e-01 -5.2785e-02\n",
      " -5.5981e-01 -3.9580e-01 -7.9849e-02 -1.0933e-02 -4.1722e-02 -5.5576e-01\n",
      "  8.8707e-02  1.3710e-01 -2.9873e-03 -2.6256e-02  7.7330e-02  3.9199e-01\n",
      "  3.4507e-01 -8.0130e-02  3.3451e-01  2.7063e-01 -2.4544e-02  7.2576e-02\n",
      " -1.8120e-01  2.3693e-01  3.9977e-01  4.5012e-01  2.7179e-02  2.7400e-01\n",
      "  1.4791e-01 -5.8324e-03  9.5910e-01 -1.0129e+00  2.0699e-01  1.8237e-01\n",
      " -2.5234e-01 -2.6261e-01 -3.4799e-01 -2.4051e-02  4.4470e-01  5.9226e-02\n",
      "  4.5561e-01  1.9700e-01 -4.8327e-01  8.9523e-02 -2.2373e-01 -1.5654e-01\n",
      "  2.1578e-01  1.1673e-01  8.2006e-02 -8.0735e-01  2.3903e-01 -5.1304e-01\n",
      " -3.3888e-01 -3.1499e-01 -1.7272e-01 -6.7020e-01  2.7096e-01 -4.3241e-01\n",
      "  4.3103e-02  2.1233e-02  1.3350e-02 -6.3938e-02 -2.4957e-01 -2.4938e-01\n",
      "  3.4812e-01 -7.1321e-02  2.3375e-01 -9.5384e-02  5.2488e-01  6.8175e-01\n",
      " -1.0214e-01 -1.4914e-01 -7.5697e-02  1.7248e-01  2.5440e-01  1.5760e-01\n",
      " -5.9125e-01  2.4300e-01  6.3962e-01 -9.3280e-02 -2.7914e-01 -6.6262e-02\n",
      " -6.7170e-02 -4.0929e-01 -3.0300e+00  1.8250e-01  2.0113e-01  6.0628e-02\n",
      " -2.4769e-01  5.5324e-02 -4.9106e-01  3.1544e-01 -3.4231e-01 -6.3766e-01\n",
      " -3.6129e-01 -5.9029e-02  1.5510e-01  4.4577e-02  2.3572e-01 -1.7095e-01\n",
      " -2.2749e-01 -2.3184e-02  2.3868e-01  2.8170e-02  4.2965e-01 -1.2458e-01\n",
      " -3.6972e-02  2.0061e-01 -3.1405e-01 -8.5287e-02 -3.3496e-01 -9.7047e-02\n",
      " -1.4388e-01  1.1147e-01 -4.5232e-01 -2.4217e-01 -1.8245e-01 -6.7292e-01\n",
      "  2.1933e-02 -5.4816e-02 -4.6508e-01  4.7767e-01 -2.4752e-01 -1.5790e-01\n",
      "  1.1817e-01  5.6851e-02 -4.9151e-01  1.5496e-01  1.6425e-02  4.1650e-02\n",
      " -3.4990e-01 -1.5979e-01  3.9705e-01  2.2963e-01  2.4688e-01  1.9567e-02\n",
      " -2.8802e-01 -6.9983e-01  3.2744e-01  1.0833e-01  2.4945e-01 -7.8653e-01\n",
      " -6.1379e-02 -3.7359e-01 -1.1603e-01 -2.4950e-01  1.0161e-01  3.3994e-02\n",
      "  1.5650e-01  2.1344e-01 -1.1094e-01 -5.7687e-03  1.7869e-01 -1.0127e-01\n",
      " -1.6891e-02  3.0001e-01 -3.4116e-01 -3.2390e-02  4.2514e-02  1.1850e-01\n",
      " -1.8337e-01 -6.2865e-01 -2.8021e-01  4.2351e-01  1.1277e-01  1.2121e-03\n",
      "  1.5710e-01 -3.6321e-01 -4.9251e-01  1.1653e-01  2.4024e-01  1.7712e-01\n",
      "  6.8700e-02 -4.4137e-01 -2.9877e-01 -1.2071e-02  2.8325e-01  1.0668e-01\n",
      " -1.8859e-01 -4.1345e-01 -3.4090e-01  4.7236e-02 -3.8309e-01  4.3572e-01\n",
      "  2.4505e-01  2.7337e-01 -7.3038e-02  4.2514e-01 -3.2455e-02 -3.5211e-01\n",
      "  4.5691e-01  1.9433e-01 -1.5230e-01  4.2675e-01  2.8795e-01 -5.5969e-01\n",
      " -1.3031e-01  8.9844e-02  4.2605e-01 -1.9632e-01 -7.1989e-02 -8.0189e-02\n",
      " -3.0425e-01 -4.6190e-01  2.8178e-01 -9.9872e-02  3.5097e-01  1.6123e-01\n",
      " -3.6548e-02 -3.6739e-01 -1.9819e-02  3.2130e-01  1.7479e-01  2.5175e-01\n",
      " -7.6439e-03 -9.3786e-02 -3.7852e-01  4.3725e-01  2.1288e-01  2.5096e-01\n",
      " -1.9613e-01 -2.8865e-01 -5.6726e-03  4.2795e-01  2.0625e-01 -3.7701e-02\n",
      " -1.2200e-01 -7.9253e-02 -1.0290e-01  1.0558e-02  4.9880e-01  2.5382e-01\n",
      "  1.5526e-01  1.7951e-03  1.1633e-01  7.9300e-02 -3.9142e-01 -3.2483e-01\n",
      "  6.3451e-01 -1.8910e-01  5.4050e-02  1.6495e-01  1.8757e-01  5.3874e-01]\n",
      "[-8.1760e-02  9.5050e-01 -1.9627e-01  3.2322e-01 -1.1475e-01  7.2491e-01\n",
      "  1.9991e-01 -4.1873e-01 -2.1523e-01  2.2864e+00 -3.7111e-01  2.0624e-01\n",
      " -3.4831e-01 -2.4620e-01  1.9677e-01 -7.9295e-02 -3.6580e-01  1.1856e+00\n",
      " -3.5559e-03  5.7556e-01 -2.3356e-01 -4.5721e-01  1.8607e-01  5.3613e-01\n",
      " -5.5108e-02  2.6244e-01 -1.9040e-02 -5.3399e-02  9.9329e-01 -4.6326e-02\n",
      " -5.8343e-01  2.4831e-01  2.3628e-01  3.6735e-01 -4.1087e-01 -2.4192e-02\n",
      " -3.8454e-01  3.3113e-01 -2.5281e-01 -2.1088e-02 -2.9030e-01 -3.1195e-01\n",
      " -3.7847e-01  7.9862e-01  3.3756e-01  6.0462e-01  2.1106e-01  1.7161e-02\n",
      "  1.6436e-01 -3.4147e-01  3.2015e-01  3.4738e-01 -6.0944e-01  2.8853e-01\n",
      "  4.7932e-01  5.8479e-01 -7.5082e-01 -4.4402e-01  6.5224e-03 -1.0748e-01\n",
      "  9.1903e-02 -1.1566e+00 -7.5945e-02 -1.7315e-01 -6.5896e-01 -7.6838e-01\n",
      "  6.9863e-01 -3.9328e-02  3.5764e-01  5.6520e-01  2.0184e-02 -4.9523e-01\n",
      " -1.9349e-01 -1.6587e-01  8.3815e-01  5.0432e-01  5.3014e-01  2.2458e-01\n",
      " -3.8416e-01  6.2961e-01  1.9621e-01  5.1145e-02  8.5893e-02 -3.1473e-01\n",
      "  3.4123e-01 -1.9530e-01  5.7403e-01 -8.9242e-01  6.0832e-01 -2.1274e-02\n",
      "  2.0518e-01 -3.1363e-01 -8.1625e-01  2.9493e-01  3.4002e-01  2.3381e-01\n",
      "  4.7663e-01  6.4454e-02 -1.7138e-01 -1.0685e-01 -1.1225e-02 -5.8588e-02\n",
      " -2.0039e-01 -2.4616e-01  3.6297e-01  5.6525e-01 -5.1624e-01 -8.0988e-01\n",
      "  2.7651e-02  1.0138e-01 -2.4744e-01 -3.0343e-01  3.0367e-01 -4.2305e-01\n",
      "  5.6549e-01 -1.1670e-01 -3.1203e-01 -5.6972e-01 -8.0784e-02 -9.2206e-02\n",
      "  1.1807e-01 -3.7990e-01  4.1985e-01 -2.6395e-01  5.8506e-01 -3.6453e-02\n",
      " -3.0516e-02 -1.4047e-01 -4.9372e-01  5.1860e-02  1.0483e-01 -4.4887e-01\n",
      " -6.6936e-01 -8.9778e-02  1.3845e-01 -6.9786e-01 -4.3077e-01  2.3854e-01\n",
      " -1.3252e-01  2.1398e-01 -1.3762e+00  1.1814e-01  1.6111e-01  1.2531e-01\n",
      "  5.4347e-02  5.4878e-01 -3.7092e-01 -1.0099e-01 -7.2403e-02 -6.6552e-01\n",
      " -3.4027e-01  3.2232e-02 -7.6320e-02  3.7552e-01 -4.1588e-01  2.2676e-01\n",
      "  3.4596e-01 -3.3434e-01 -5.3742e-01  9.4397e-02 -1.7104e-02 -2.6160e-01\n",
      "  2.7804e-01 -6.5511e-02  7.3646e-02  2.5776e-01 -3.5709e-02 -1.4276e-01\n",
      " -2.0539e-01 -1.8760e-01 -5.5609e-01 -3.2273e-01  2.9594e-01 -3.9765e-01\n",
      " -2.5016e-02 -4.8199e-02 -3.4339e-01 -1.7692e-01  2.5867e-01  6.8487e-01\n",
      " -3.3053e-01 -5.7565e-02 -3.3737e-01 -1.2563e-01 -2.9366e-01  1.9139e-01\n",
      "  5.3887e-02 -2.7826e-02  1.9512e-01  1.1829e-01 -5.8045e-01  3.5679e-01\n",
      " -6.1477e-01 -5.1816e-01  1.4871e-01  8.7451e-01 -2.3132e-01  4.0470e-01\n",
      " -7.8567e-02 -2.0145e-01  4.3885e-01 -1.0504e-01  6.6496e-02  1.5818e-01\n",
      "  3.2686e-01  7.7705e-02 -1.3774e-01 -2.8313e-01  5.6313e-02  3.0749e-02\n",
      " -1.6830e-01 -2.4820e-01  1.6009e-01 -2.7654e-01  2.6247e-01 -7.1188e-03\n",
      " -3.4712e-01  2.9753e-01 -3.5213e-01 -3.3462e-02  3.2864e-01 -4.1862e-02\n",
      "  3.7404e-01  2.5576e-01  1.6339e-01 -7.5334e-02  3.4678e-01  1.8152e-01\n",
      " -3.8499e-01 -3.7584e-01 -9.0580e-02  1.9120e-01 -3.0927e-01  1.3177e-01\n",
      " -2.7881e-01 -5.6982e-01 -6.6286e-01 -3.0949e-01 -2.0002e-01  4.9368e-01\n",
      "  3.4719e-01  2.3746e-02  1.0352e-01  2.8529e-01 -4.0391e-01 -4.0647e-01\n",
      " -3.9268e-02 -1.4621e-01  3.4379e-01  8.5500e-01 -7.4400e-02  1.5851e-01\n",
      " -7.3204e-02 -7.2655e-02  6.4226e-01 -2.3160e-01  7.3683e-03 -9.2140e-02\n",
      "  6.3533e-02 -7.4703e-02  3.4700e-01  4.2235e-01  4.8242e-01  1.7781e-01\n",
      " -1.4147e-01 -1.8437e-01 -2.4470e-01  8.4425e-02 -1.0549e-03  1.0658e-01\n",
      " -2.5153e-01 -1.7086e-01 -6.8509e-01 -1.2899e-01  8.3986e-02  1.5304e-01\n",
      "  1.1152e-01 -4.2578e-01 -3.4582e-01  7.9028e-01  2.5903e-01 -5.1056e-01\n",
      "  7.9888e-02 -2.5027e-02  1.8446e-01  2.6668e-02  2.2356e-01 -1.5973e-01\n",
      "  1.0135e-01  8.0687e-01  6.3786e-02  2.2335e-01 -2.6171e-01 -4.6166e-01\n",
      "  3.7709e-02 -7.7545e-02 -1.7156e-01 -1.5184e-01 -2.3864e-01  4.1293e-01]\n",
      "[ 0.036775   0.40917   -0.52141   -0.067184   0.087702  -0.048564\n",
      "  0.40947   -0.42818    0.19304    2.3925    -0.11441   -0.22952\n",
      " -0.16061    0.035533  -0.53179    0.19764   -0.48827    0.57439\n",
      " -0.064301   0.47053   -0.29647   -0.15927   -0.052798   0.10121\n",
      " -0.054461   0.036129  -0.16118   -0.34139    0.45834   -0.20144\n",
      " -0.29067   -0.51888   -0.062106   0.14084    0.016413   0.050826\n",
      "  0.13243   -0.033663  -0.42228   -0.30086    0.06202    0.26338\n",
      "  0.077223   0.27307    0.13392    0.30183   -0.16546    0.057011\n",
      " -0.0034585 -0.071113  -0.27287   -0.10297    0.07457   -0.32104\n",
      "  0.36696    0.27051   -0.15776    0.2978    -0.18988    0.097477\n",
      "  0.035665  -0.49749   -0.52759   -0.046148   0.021715  -0.11047\n",
      " -0.18007    0.20295    0.15254   -0.045976  -0.21846   -0.066865\n",
      " -0.21355    0.017509   0.66474    0.25527    0.24864   -0.094851\n",
      " -0.012857   0.46896    0.052031   0.62488   -0.12662    0.063972\n",
      " -0.15719   -0.45907    0.32286   -0.17502    0.64181    0.091587\n",
      " -0.075871   0.11718   -0.13864    0.24951   -0.40664    0.08845\n",
      " -0.29196   -0.51624   -0.074847  -0.012822  -0.088844  -0.19935\n",
      "  0.052734  -0.13588    0.231     -0.34368    0.30607   -0.21223\n",
      "  0.08178    0.10097    0.33585   -0.17491    0.019115   0.15998\n",
      "  0.38803   -0.35932    0.31682   -0.18614    0.11732   -0.068517\n",
      "  0.50785   -0.0035486  0.20069    0.25218    0.38309    0.19359\n",
      "  0.43857   -0.29954   -0.14219    0.087962  -0.14229    0.10075\n",
      " -0.58986   -0.12672    0.036944  -0.050421  -0.19875   -0.051368\n",
      " -0.023402   0.08744   -2.4938     0.15427    0.12373   -0.0086429\n",
      " -0.17007   -0.519     -0.29962    0.24369   -0.20535   -0.24942\n",
      " -0.079362   0.40986   -0.10753    0.098907  -0.063449   0.05373\n",
      "  0.26206    0.13207   -0.067694  -0.56168   -0.18867    0.14453\n",
      " -0.22469   -0.28404    0.20909   -0.46989    0.30992   -0.13283\n",
      "  0.041392   0.11146    0.17015   -0.059407  -0.16098   -0.2211\n",
      " -0.0035877 -0.22357   -0.01852   -0.23026   -0.18824    0.32997\n",
      "  0.16287   -0.52067    0.17308   -0.024264  -0.041321  -0.3241\n",
      " -0.44122   -0.11114    0.22684   -0.10883   -0.1278    -0.16696\n",
      "  0.051048  -0.12131    0.18038    0.19793    0.134     -0.37113\n",
      "  0.36008    0.092685  -0.30263    0.16565   -0.10863   -0.29565\n",
      "  0.26143    0.13369   -0.090181   0.021989  -0.093353  -0.20325\n",
      " -0.2008     0.20721    0.17208   -0.20199    0.043315   0.17768\n",
      "  0.57448   -0.45917   -0.077197   0.12051    0.07209   -0.095313\n",
      "  0.10973    0.22375    0.045804  -0.13573    0.14041   -0.11364\n",
      " -0.46605   -0.43262   -0.058678   0.19043   -0.40867    0.30509\n",
      "  0.18542    0.095309  -0.42329   -0.15225   -0.13827    0.18119\n",
      "  0.14755   -0.053628   0.031298   0.65695   -0.1717     0.23649\n",
      " -0.34742   -0.17438   -0.085304   0.37687    0.21322   -0.13184\n",
      " -0.35197   -0.14072    0.2332     0.21014   -0.14763    0.047515\n",
      " -0.27979    0.090331  -0.15565    0.42803   -0.019297   0.012198\n",
      "  0.036031  -0.10396    0.11014    0.13458    0.2775     0.36225\n",
      " -0.35591   -0.16877   -0.41201    0.070133  -0.27769    0.13739\n",
      " -0.057831   0.19277    0.11131    0.53696    0.0093424 -0.26107\n",
      " -0.38663    0.040653   0.18617    0.26312    0.12212   -0.030012\n",
      "  0.096286   0.47376   -0.21633    0.10798   -0.17703    0.22116\n",
      "  0.6726     0.065036  -0.017414  -0.048585  -0.090863   0.28591  ]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp('I am happy')\n",
    "\n",
    "# Generate word vectors for each token\n",
    "for token in doc:\n",
    "    print(token.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarities\n",
    "\n",
    "We can compute how similar two words are to each other by using the **similarity** method of a **spaCy token**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy happy 1.0\n",
      "happy joyous 0.533303\n",
      "happy sad 0.64389884\n",
      "joyous happy 0.533303\n",
      "joyous joyous 1.0\n",
      "joyous sad 0.43832767\n",
      "sad happy 0.64389884\n",
      "sad joyous 0.43832767\n",
      "sad sad 1.0\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"happy joyous sad\")\n",
    "\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, 'happy' and 'joyous' are more similar to each other than they are to 'sad'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document similarities\n",
    "\n",
    "**spaCy** also allows us to directly compute the similarity between two documents by using the average of the word vectors of all the words in a particular document.\n",
    "\n",
    "Like **spaCy tokens**, **docs** also have a **similarity** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9492464724721577\n",
      "0.9239675481730458\n"
     ]
    }
   ],
   "source": [
    "# Generate doc objects\n",
    "sent1 = nlp(\"I am happy\")\n",
    "sent2 = nlp(\"I am sad\")\n",
    "sent3 = nlp(\"I am joyous\")\n",
    "\n",
    "# Compute similarity between sent1 and sent2\n",
    "print(sent1.similarity(sent2))\n",
    "\n",
    "# Compute similarity between sent1 and sent3\n",
    "print(sent1.similarity(sent3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, \"I am happy\" is more similar to \"I am joyous\" than it is to \"I am sad\". Note that the similarity scores are high in both cases, because all sentences share two out of their three words, 'I' and 'am'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Generating word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I 1.0\n",
      "I like 0.55549127\n",
      "I apples 0.20442723\n",
      "I and 0.31607857\n",
      "I oranges 0.18824081\n",
      "like I 0.55549127\n",
      "like like 1.0\n",
      "like apples 0.32987145\n",
      "like and 0.5267485\n",
      "like oranges 0.27717474\n",
      "apples I 0.20442723\n",
      "apples like 0.32987145\n",
      "apples apples 1.0\n",
      "apples and 0.2409773\n",
      "apples oranges 0.77809423\n",
      "and I 0.31607857\n",
      "and like 0.5267485\n",
      "and apples 0.2409773\n",
      "and and 1.0\n",
      "and oranges 0.19245945\n",
      "oranges I 0.18824081\n",
      "oranges like 0.27717474\n",
      "oranges apples 0.77809423\n",
      "oranges and 0.19245945\n",
      "oranges oranges 1.0\n"
     ]
    }
   ],
   "source": [
    "sent = 'I like apples and oranges'\n",
    "\n",
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! Notice how the words '<code style=\"background:#RRGGBB\">apples</code>' and '<code style=\"background:#RRGGBB\">oranges</code>' have the highest pairwaise similarity score. This is expected as they are both fruits and are more related to each other than any other pair of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Computing similarity of Pink Floyd songs\n",
    "\n",
    "In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely 'High Hopes', 'Hey You' and 'Mother'. The lyrics to these songs are available as <code style=\"background:#RRGGBB\">hopes</code>, <code style=\"background:#RRGGBB\">hey</code> and <code style=\"background:#RRGGBB\">mother</code> respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8653562508450858\n",
      "0.9595267703981097\n"
     ]
    }
   ],
   "source": [
    "hopes = \"\\nBeyond the horizon of the place we lived when we were young\\nIn a world of magnets and miracles\\nOur thoughts strayed constantly and without boundary\\nThe ringing of the division bell had begun\\nAlong the Long Road and on down the Causeway\\nDo they still meet there by the Cut\\nThere was a ragged band that followed in our footsteps\\nRunning before times took our dreams away\\nLeaving the myriad small creatures trying to tie us to the ground\\nTo a life consumed by slow decay\\nThe grass was greener\\nThe light was brighter\\nWhen friends surrounded\\nThe nights of wonder\\nLooking beyond the embers of bridges glowing behind us\\nTo a glimpse of how green it was on the other side\\nSteps taken forwards but sleepwalking back again\\nDragged by the force of some in a tide\\nAt a higher altitude with flag unfurled\\nWe reached the dizzy heights of that dreamed of world\\nEncumbered forever by desire and ambition\\nThere's a hunger still unsatisfied\\nOur weary eyes still stray to the horizon\\nThough down this road we've been so many times\\nThe grass was greener\\nThe light was brighter\\nThe taste was sweeter\\nThe nights of wonder\\nWith friends surrounded\\nThe dawn mist glowing\\nThe water flowing\\nThe endless river\\nForever and ever\\n\"\n",
    "hey = \"\\nHey you, out there in the cold\\nGetting lonely, getting old\\nCan you feel me?\\nHey you, standing in the aisles\\nWith itchy feet and fading smiles\\nCan you feel me?\\nHey you, don't help them to bury the light\\nDon't give in without a fight\\nHey you out there on your own\\nSitting naked by the phone\\nWould you touch me?\\nHey you with you ear against the wall\\nWaiting for someone to call out\\nWould you touch me?\\nHey you, would you help me to carry the stone?\\nOpen your heart, I'm coming home\\nBut it was only fantasy\\nThe wall was too high\\nAs you can see\\nNo matter how he tried\\nHe could not break free\\nAnd the worms ate into his brain\\nHey you, out there on the road\\nAlways doing what you're told\\nCan you help me?\\nHey you, out there beyond the wall\\nBreaking bottles in the hall\\nCan you help me?\\nHey you, don't tell me there's no hope at all\\nTogether we stand, divided we fall\\n\"\n",
    "mother = \"\\nMother do you think they'll drop the bomb?\\nMother do you think they'll like this song?\\nMother do you think they'll try to break my balls?\\nOoh, ah\\nMother should I build the wall?\\nMother should I run for President?\\nMother should I trust the government?\\nMother will they put me in the firing mine?\\nOoh ah,\\nIs it just a waste of time?\\nHush now baby, baby, don't you cry.\\nMama's gonna make all your nightmares come true.\\nMama's gonna put all her fears into you.\\nMama's gonna keep you right here under her wing.\\nShe won't let you fly, but she might let you sing.\\nMama's gonna keep baby cozy and warm.\\nOoh baby, ooh baby, ooh baby,\\nOf course mama's gonna help build the wall.\\nMother do you think she's good enough, for me?\\nMother do you think she's dangerous, to me?\\nMother will she tear your little boy apart?\\nOoh ah,\\nMother will she break my heart?\\nHush now baby, baby don't you cry.\\nMama's gonna check out all your girlfriends for you.\\nMama won't let anyone dirty get through.\\nMama's gonna wait up until you get in.\\nMama will always find out where you've been.\\nMama's gonna keep baby healthy and clean.\\nOoh baby, ooh baby, ooh baby,\\nYou'll always be baby to me.\\nMother, did it need to be so high?\\n\"\n",
    "\n",
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! Notice that 'Mother' and 'Hey You' have a similarity score of 0.9 whereas 'Mother' and 'High Hopes' has a score of only 0.6. This is probably because 'Mother' and 'Hey You' were both songs from the same album 'The Wall' and were penned by Roger Waters. On the other hand, 'High Hopes' was a part of the album 'Division Bell' with lyrics by David Gilmour and his wife, Penny Samson. Treat yourself by listening to these songs. They're some of the best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "- Basic features (characters, words, mentions, etc.)\n",
    "- Readability scores\n",
    "- Tokenization and lemmatization\n",
    "- Text cleaning\n",
    "- Part-of-speech tagging & named entity recognition\n",
    "- n-gram modeling\n",
    "- tf-idf\n",
    "- Cosine similarity\n",
    "- Word embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
